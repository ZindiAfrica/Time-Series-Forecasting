{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Layer.ai Air Quality Prediction Challenge 1st Place Solution</center></h1>"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "lRxz1LX1uPcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE OF CONTENTS**\n",
        "\n",
        "<ul class=\"list-group\" style=\"list-style-type:none;\">\n",
        "    <li>1. Setup </li>\n",
        "    <li>2. Load Data </li>\n",
        "    <li>3. Data Cleaning </li>\n",
        "    <li>4. Feature Engineering </li>\n",
        "    <li>5. Preprocessing </li>\n",
        "    <li>6. Modeling </li>\n",
        "    <ul class=\"list-group2\" style=\"list-style-type:none;\">\n",
        "        <li>A. Define Model </li>\n",
        "        <li>B. Tune Hyperparameters </li>\n",
        "        <li>C. Model Training </li>\n",
        "        <li>D. Model Evaluation </li>\n",
        "        <li>E. Error Analysis </li>\n",
        "        <li>F. Inference </li>\n",
        "    </ul>\n",
        "    <li>7. Submission </li>\n",
        "</ul>\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "K3s86uOauPcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Links to datasets used:\n",
        "1. Imputed train set can be found [here](https://app.layer.ai/nkosana_mlandu/zindi-air-quality-prediction-challenge/datasets/imputed_train)\n",
        "2. Imputed test set can be found [here](https://app.layer.ai/nkosana_mlandu/zindi-air-quality-prediction-challenge/datasets/imputed_test)\n",
        "3. Pseudo labels can be find [here](https://app.layer.ai/nkosana_mlandu/zindi-air-quality-prediction-challenge/datasets/pseudo_labels?v=1.1)"
      ],
      "metadata": {
        "id": "kkF0EOS5erFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "k8YQnBeXuPcg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:21:58.640517Z",
          "iopub.status.busy": "2022-10-01T18:21:58.639405Z",
          "iopub.status.idle": "2022-10-01T18:22:33.034612Z",
          "shell.execute_reply": "2022-10-01T18:22:33.032888Z",
          "shell.execute_reply.started": "2022-10-01T18:21:58.640465Z"
        },
        "id": "MOhGB-t76QLJ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Install the latest version of layer\n",
        "!pip install layer --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install pytorch_forecasting --upgrade"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Uf-j0ZltuPcq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yo4GkrVLuPcu"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch-forecasting==0.9.0 && pip install pytorch-lightning==1.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:33.038636Z",
          "iopub.status.busy": "2022-10-01T18:22:33.038226Z",
          "iopub.status.idle": "2022-10-01T18:22:36.584879Z",
          "shell.execute_reply": "2022-10-01T18:22:36.583331Z",
          "shell.execute_reply.started": "2022-10-01T18:22:33.038591Z"
        },
        "id": "_PUnTT0G6QLM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import layer\n",
        "from layer.decorators import dataset, model, resources\n",
        "\n",
        "# Inorder to use layer please provide your own API KEY\n",
        "# Get your API KEY here:\n",
        "# https://app.layer.ai/me/settings/developer\n",
        "API_KEY = \"ENTER-API-KEY\"\n",
        "layer.login_with_api_key(API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:36.587589Z",
          "iopub.status.busy": "2022-10-01T18:22:36.586672Z",
          "iopub.status.idle": "2022-10-01T18:22:37.545333Z",
          "shell.execute_reply": "2022-10-01T18:22:37.543974Z",
          "shell.execute_reply.started": "2022-10-01T18:22:36.587537Z"
        },
        "id": "8Wj4UdrU6QLO",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Initialise project\n",
        "layer.init(\"zindi-air-quality-prediction-challenge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:37.549958Z",
          "iopub.status.busy": "2022-10-01T18:22:37.548731Z",
          "iopub.status.idle": "2022-10-01T18:22:37.558759Z",
          "shell.execute_reply": "2022-10-01T18:22:37.557549Z",
          "shell.execute_reply.started": "2022-10-01T18:22:37.549916Z"
        },
        "id": "CFpl0KMg6QLQ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Import core packages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import pytorch_forecasting\n",
        "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
        "\n",
        "import random\n",
        "import seaborn as sns\n",
        "import sys\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import SplineTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "\n",
        "# pd.set_option('display.max_rows', 1000)           \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:37.560523Z",
          "iopub.status.busy": "2022-10-01T18:22:37.560153Z",
          "iopub.status.idle": "2022-10-01T18:22:37.576587Z",
          "shell.execute_reply": "2022-10-01T18:22:37.575148Z",
          "shell.execute_reply.started": "2022-10-01T18:22:37.560489Z"
        },
        "id": "7G2WXLHw6QLV",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed, pytorch_init=True):\n",
        "    \"\"\"\n",
        "    Seeds basic parameters for reproducibility of results\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    if pytorch_init is True:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:37.579548Z",
          "iopub.status.busy": "2022-10-01T18:22:37.578645Z",
          "iopub.status.idle": "2022-10-01T18:22:37.588752Z",
          "shell.execute_reply": "2022-10-01T18:22:37.587859Z",
          "shell.execute_reply.started": "2022-10-01T18:22:37.579504Z"
        },
        "id": "GByjjwJh6QLd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "SEED = 2047\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7KGvv7s6QLe",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2GRJXQdNPHm",
        "scrolled": true,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train = layer.get_dataset(\"nkosana_mlandu/zindi-air-quality-prediction-challenge/datasets/imputed_train:1.1\").to_pandas()\n",
        "test = layer.get_dataset(\"nkosana_mlandu/zindi-air-quality-prediction-challenge/datasets/imputed_test:1.1\").to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:38.003961Z",
          "iopub.status.busy": "2022-10-01T18:22:38.003541Z",
          "iopub.status.idle": "2022-10-01T18:22:38.009003Z",
          "shell.execute_reply": "2022-10-01T18:22:38.007477Z",
          "shell.execute_reply.started": "2022-10-01T18:22:38.003928Z"
        },
        "id": "_mwi0YTB6QLi",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# # train = layer.get_dataset(\"nkosana_mlandu/zindi-air-quality-prediction-challenge/datasets/competition-data-train:1.1\").to_pandas()\n",
        "# # test = layer.get_dataset(\"nkosana_mlandu/zindi-air-quality-prediction-challenge/datasets/competition-data-test:1.1\").to_pandas()\n",
        "sample_submission = layer.get_dataset(\"nkosana_mlandu/zindi-air-quality-prediction-challenge/datasets/competition-data-samplesubmission:1.1\").to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:38.011815Z",
          "iopub.status.busy": "2022-10-01T18:22:38.011282Z",
          "iopub.status.idle": "2022-10-01T18:22:38.036871Z",
          "shell.execute_reply": "2022-10-01T18:22:38.035485Z",
          "shell.execute_reply.started": "2022-10-01T18:22:38.011751Z"
        },
        "id": "T0kId4f76QLj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Check for any missing values\n",
        "train.isnull().sum().any(), test.isnull().sum().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:38.038371Z",
          "iopub.status.busy": "2022-10-01T18:22:38.038000Z",
          "iopub.status.idle": "2022-10-01T18:22:38.046126Z",
          "shell.execute_reply": "2022-10-01T18:22:38.044748Z",
          "shell.execute_reply.started": "2022-10-01T18:22:38.038339Z"
        },
        "id": "_t4iuHzI6QLk",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def check_missing_values(df):\n",
        "    # Get count of missing values\n",
        "    total = df.isnull().sum()\n",
        "    # Get percentage of missing values\n",
        "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
        "    missing_value_df = pd.DataFrame({'percent_missing': percent_missing,\n",
        "                                    'total': total,\n",
        "                                    'type': df.dtypes})\n",
        "    # Sort by pct\n",
        "    missing_value_df.sort_values('percent_missing', ascending=False, inplace=True)\n",
        "    \n",
        "    return missing_value_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:38.047924Z",
          "iopub.status.busy": "2022-10-01T18:22:38.047547Z",
          "iopub.status.idle": "2022-10-01T18:22:38.096751Z",
          "shell.execute_reply": "2022-10-01T18:22:38.095505Z",
          "shell.execute_reply.started": "2022-10-01T18:22:38.047892Z"
        },
        "id": "nja7F7i_6QLk",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Check for missing value pct in train\n",
        "check_missing_values(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:38.099448Z",
          "iopub.status.busy": "2022-10-01T18:22:38.098914Z",
          "iopub.status.idle": "2022-10-01T18:22:38.126518Z",
          "shell.execute_reply": "2022-10-01T18:22:38.125330Z",
          "shell.execute_reply.started": "2022-10-01T18:22:38.099399Z"
        },
        "id": "POG0jyRy6QLl",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Check for missing value pct in test\n",
        "check_missing_values(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmKYAFLS6QLl",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:38.129392Z",
          "iopub.status.busy": "2022-10-01T18:22:38.128140Z",
          "iopub.status.idle": "2022-10-01T18:22:38.223963Z",
          "shell.execute_reply": "2022-10-01T18:22:38.222658Z",
          "shell.execute_reply.started": "2022-10-01T18:22:38.129347Z"
        },
        "id": "gsiXJhjS6QLm",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "train.duplicated().any(), test.duplicated().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1cJx80w6QLm",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:38.227786Z",
          "iopub.status.busy": "2022-10-01T18:22:38.227380Z",
          "iopub.status.idle": "2022-10-01T18:22:38.235047Z",
          "shell.execute_reply": "2022-10-01T18:22:38.233791Z",
          "shell.execute_reply.started": "2022-10-01T18:22:38.227751Z"
        },
        "id": "tFAjiyxa6QLm",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train.shape, test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:38.237667Z",
          "iopub.status.busy": "2022-10-01T18:22:38.237180Z",
          "iopub.status.idle": "2022-10-01T18:22:38.471276Z",
          "shell.execute_reply": "2022-10-01T18:22:38.470125Z",
          "shell.execute_reply.started": "2022-10-01T18:22:38.237621Z"
        },
        "id": "6DRUwHey6QLn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:38.473200Z",
          "iopub.status.busy": "2022-10-01T18:22:38.472805Z",
          "iopub.status.idle": "2022-10-01T18:22:38.659684Z",
          "shell.execute_reply": "2022-10-01T18:22:38.658228Z",
          "shell.execute_reply.started": "2022-10-01T18:22:38.473164Z"
        },
        "id": "XBBHRZlm6QLn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "test.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:38.661749Z",
          "iopub.status.busy": "2022-10-01T18:22:38.661113Z",
          "iopub.status.idle": "2022-10-01T18:22:39.189179Z",
          "shell.execute_reply": "2022-10-01T18:22:39.188146Z",
          "shell.execute_reply.started": "2022-10-01T18:22:38.661717Z"
        },
        "id": "G0y0L7Ua6QLo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "sns.histplot(data=train, x=\"pm2_5\", color=\"blue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:39.191673Z",
          "iopub.status.busy": "2022-10-01T18:22:39.190634Z",
          "iopub.status.idle": "2022-10-01T18:22:39.618873Z",
          "shell.execute_reply": "2022-10-01T18:22:39.617490Z",
          "shell.execute_reply.started": "2022-10-01T18:22:39.191627Z"
        },
        "id": "oNhgyDlR6QLo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# plot distribution after log transformation\n",
        "sns.histplot(np.log(train['pm2_5']), color=\"blue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:39.621129Z",
          "iopub.status.busy": "2022-10-01T18:22:39.620745Z",
          "iopub.status.idle": "2022-10-01T18:22:40.116197Z",
          "shell.execute_reply": "2022-10-01T18:22:40.114803Z",
          "shell.execute_reply.started": "2022-10-01T18:22:39.621095Z"
        },
        "id": "9KGKVbDY6QLp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "transformer = PowerTransformer(method=\"box-cox\", standardize=False)\n",
        "sns.histplot(transformer.fit_transform(train['pm2_5'].values.reshape(-1, 1)), color=\"blue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:40.119149Z",
          "iopub.status.busy": "2022-10-01T18:22:40.118209Z",
          "iopub.status.idle": "2022-10-01T18:22:40.127497Z",
          "shell.execute_reply": "2022-10-01T18:22:40.126199Z",
          "shell.execute_reply.started": "2022-10-01T18:22:40.119096Z"
        },
        "id": "Du3DDoHh6QLp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def get_data_collection_report(df, kind=\"train\"):\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    \n",
        "    start_date = df[\"date\"].min()\n",
        "    end_date = df[\"date\"].max()\n",
        "    duration = end_date - start_date\n",
        "    \n",
        "    print(f\"Data in {kind} covers the period {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "    print(f\"Length of period: {duration.days + 1} days\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:40.129733Z",
          "iopub.status.busy": "2022-10-01T18:22:40.129219Z",
          "iopub.status.idle": "2022-10-01T18:22:40.150332Z",
          "shell.execute_reply": "2022-10-01T18:22:40.148808Z",
          "shell.execute_reply.started": "2022-10-01T18:22:40.129683Z"
        },
        "id": "ALM6ocSd6QLp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# print a report of collection dates for train set\n",
        "get_data_collection_report(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:40.152257Z",
          "iopub.status.busy": "2022-10-01T18:22:40.151858Z",
          "iopub.status.idle": "2022-10-01T18:22:40.164893Z",
          "shell.execute_reply": "2022-10-01T18:22:40.163388Z",
          "shell.execute_reply.started": "2022-10-01T18:22:40.152221Z"
        },
        "id": "Cv40aT-26QLq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# print a report of collection dates for test set\n",
        "get_data_collection_report(test, kind=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:40.167470Z",
          "iopub.status.busy": "2022-10-01T18:22:40.166790Z",
          "iopub.status.idle": "2022-10-01T18:22:40.173629Z",
          "shell.execute_reply": "2022-10-01T18:22:40.172529Z",
          "shell.execute_reply.started": "2022-10-01T18:22:40.167419Z"
        },
        "id": "NmqRZNvi6QLq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# # drop pm2_5 values above 300\n",
        "# train = train[(train.pm2_5 <= 200) & (train.pm2_5 >= 10)]\n",
        "# train = train.reset_index().drop(columns=[\"index\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt2vmKCg6QLr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:40.176420Z",
          "iopub.status.busy": "2022-10-01T18:22:40.175367Z",
          "iopub.status.idle": "2022-10-01T18:22:40.185866Z",
          "shell.execute_reply": "2022-10-01T18:22:40.184482Z",
          "shell.execute_reply.started": "2022-10-01T18:22:40.176371Z"
        },
        "id": "K-IIFjixS80y",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/flaviagiammarino/brits-tensorflow.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:40.188403Z",
          "iopub.status.busy": "2022-10-01T18:22:40.187901Z",
          "iopub.status.idle": "2022-10-01T18:22:40.200641Z",
          "shell.execute_reply": "2022-10-01T18:22:40.199338Z",
          "shell.execute_reply.started": "2022-10-01T18:22:40.188351Z"
        },
        "id": "PmPUK0QITFA4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# sys.path.append(\"brits-tensorflow\")\n",
        "\n",
        "# from brits_tensorflow.model import BRITS\n",
        "\n",
        "\n",
        "# all_df = pd.concat([train, test])\n",
        "# all_df = all_df.set_index(\"ID\")\n",
        "\n",
        "# all_df['date'] = pd.to_datetime(all_df['date'])\n",
        "# all_df = all_df.sort_values(by=[\"site_latitude\",\"date\"])\n",
        "\n",
        "# missing = check_missing_values(train)\n",
        "# cols_with_missing = missing[(missing.percent_missing > 0)].index\n",
        "\n",
        "# num_steps = 1\n",
        "# # Fit the model\n",
        "# model = BRITS(\n",
        "#     x=all_df[cols_with_missing].to_numpy(),\n",
        "#     units=100,\n",
        "#     timesteps=num_steps)\n",
        "\n",
        "# model.fit(\n",
        "#     learning_rate=1e-4,\n",
        "#     batch_size=16,\n",
        "#     epochs=50,\n",
        "#     verbose=True)\n",
        "    \n",
        "# # Impute the missing values\n",
        "# imputations = model.predict(x=all_df[cols_with_missing].to_numpy())\n",
        "\n",
        "# # stations = list()\n",
        "\n",
        "# # for i, lat in enumerate(all_df[\"site_latitude\"].unique()):\n",
        "# #     site_data = all_df[all_df.site_latitude == lat]\n",
        "    \n",
        "# #     # Fit the model\n",
        "# #     model = BRITS(\n",
        "# #         x=site_data[cols_with_missing].to_numpy(),\n",
        "# #         units=100,\n",
        "# #         timesteps=200)\n",
        "    \n",
        "# #     model.fit(\n",
        "# #         learning_rate=0.001,\n",
        "# #         batch_size=16,\n",
        "# #         epochs=200,\n",
        "# #         verbose=False\n",
        "# #     )\n",
        "    \n",
        "# #     # Impute the missing values\n",
        "# #     imputations = model.predict(x=site_data[cols_with_missing].to_numpy())\n",
        "    \n",
        "# #     site_data[cols_with_missing] = imputations.values\n",
        "# #     stations.append(site_data)\n",
        "    \n",
        "# #     print(f'Finished imputing for station at {lat} ({i+1}/{all_df[\"site_latitude\"].nunique()})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:40.210626Z",
          "iopub.status.busy": "2022-10-01T18:22:40.210159Z",
          "iopub.status.idle": "2022-10-01T18:22:40.222080Z",
          "shell.execute_reply": "2022-10-01T18:22:40.220556Z",
          "shell.execute_reply.started": "2022-10-01T18:22:40.210591Z"
        },
        "id": "QiwK7Fmh7a1R",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# all_df[cols_with_missing] = imputations.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:40.224551Z",
          "iopub.status.busy": "2022-10-01T18:22:40.224104Z",
          "iopub.status.idle": "2022-10-01T18:22:40.236257Z",
          "shell.execute_reply": "2022-10-01T18:22:40.234758Z",
          "shell.execute_reply.started": "2022-10-01T18:22:40.224512Z"
        },
        "id": "7_U6d5yf7a1S",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# train = all_df[~all_df.pm2_5.isna()]\n",
        "# test = all_df[all_df.pm2_5.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:40.238282Z",
          "iopub.status.busy": "2022-10-01T18:22:40.237840Z",
          "iopub.status.idle": "2022-10-01T18:22:40.250680Z",
          "shell.execute_reply": "2022-10-01T18:22:40.249194Z",
          "shell.execute_reply.started": "2022-10-01T18:22:40.238244Z"
        },
        "id": "9CVQctzF7a1S",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# train.to_csv(\"train_imputed.csv\")\n",
        "# test.to_csv(\"test_imputed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:40.253370Z",
          "iopub.status.busy": "2022-10-01T18:22:40.252214Z",
          "iopub.status.idle": "2022-10-01T18:22:40.758769Z",
          "shell.execute_reply": "2022-10-01T18:22:40.757056Z",
          "shell.execute_reply.started": "2022-10-01T18:22:40.253318Z"
        },
        "id": "Zxe_pps78hC9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# train = pd.read_csv(\"../input/layerai-air-quality-prediction-challenge/train_imputed.csv\")\n",
        "# test = pd.read_csv(\"../input/layerai-air-quality-prediction-challenge/test_imputed.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaIaNeS46QLw",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "train[\"date\"] = pd.to_datetime(train[\"date\"])\n",
        "test[\"date\"] = pd.to_datetime(test[\"date\"])"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "LvZGW2ttuPeM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def sin_transformer(period):\n",
        "    return FunctionTransformer(lambda x: np.sin(x / period *2 * np.pi))\n",
        "\n",
        "def cos_transformer(period):\n",
        "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
        "\n",
        "def periodic_spline_transformer(period, n_splines=None, degree=3):\n",
        "    if n_splines is None:\n",
        "        n_splines = period\n",
        "    n_knots = n_splines + 1 # periodoc and include bias = True\n",
        "    return SplineTransformer(degree=degree,\n",
        "                            n_knots=n_knots,\n",
        "                            knots=np.linspace(0, period, n_knots).reshape(n_knots, 1),\n",
        "                            extrapolation=\"periodic\",\n",
        "                            include_bias=True)\n",
        "\n",
        "def extract_time_features(df):\n",
        "\n",
        "    # Extract raw ordinal time-related features\n",
        "    df[\"year\"] = df.date.dt.year\n",
        "    df[\"month\"] = df.date.dt.month\n",
        "    df[\"day\"] = df.date.dt.day\n",
        "    df[\"weekday\"] = df.date.dt.dayofweek\n",
        "    df[\"day_of_year\"] = df.date.dt.day_of_year\n",
        "    df[\"week_of_year\"] = df.date.dt.weekofyear\n",
        "    df[\"quarter\"] = df.date.dt.quarter\n",
        "    df[\"is_month_start\"] = df.date.dt.is_month_start\n",
        "    df[\"is_month_end\"] = df.date.dt.is_month_end\n",
        "    df[\"is_quarter_start\"] = df.date.dt.is_quarter_start\n",
        "    df[\"is_quarter_end\"] = df.date.dt.is_quarter_end\n",
        "\n",
        "\n",
        "\n",
        "    dayname = df.date.dt.day_name()\n",
        "    df[\"is_weekend\"] = np.where(dayname.isin([\"Saturday\", \"Sunday\"]), \"Yes\", \"No\")\n",
        "\n",
        "    # Extract trigonometric features\n",
        "    df[\"month_sin\"] = sin_transformer(12).fit_transform(df[\"month\"])\n",
        "    df[\"month_cos\"] = cos_transformer(12).fit_transform(df[\"month\"])\n",
        "\n",
        "    df[\"weekday_sin\"] = sin_transformer(7).fit_transform(df[\"weekday\"])\n",
        "    df[\"weekday_cos\"] = cos_transformer(7).fit_transform(df[\"weekday\"])\n",
        "\n",
        "    # Extract periodic spline-based features\n",
        "    spline_cols_month = [f\"cyclic_month_spline_{i}\" for i in range(1, 7)]\n",
        "    spline_cols_weekday = [f\"cyclic_weekday_spline_{i}\" for i in range(1, 4)]\n",
        "\n",
        "    df[spline_cols_month] = periodic_spline_transformer(12, n_splines=6).fit_transform(np.array(df.month).reshape(-1,1))\n",
        "    df[spline_cols_weekday] = periodic_spline_transformer(7, n_splines=3).fit_transform(np.array(df.weekday).reshape(-1,1))\n",
        "\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HEMUH_33uPeO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Extract date related features\n",
        "# Questions to ask: Is there any relationship between date and PM2.5 concentration?\n",
        "# Are there some trends based on date, maybe seasonality?\n",
        "train = extract_time_features(train)\n",
        "test = extract_time_features(test)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "u9dXcr-UuPeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:41.073395Z",
          "iopub.status.busy": "2022-10-01T18:22:41.072630Z",
          "iopub.status.idle": "2022-10-01T18:22:41.086493Z",
          "shell.execute_reply": "2022-10-01T18:22:41.085265Z",
          "shell.execute_reply.started": "2022-10-01T18:22:41.073354Z"
        },
        "id": "GtP4G0kP6QLw",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Extract geospatial features\n",
        "def extract_geo_spatial_feat(df):\n",
        "    # Add polar coordinates\n",
        "    df[\"polar_radius\"] = np.sqrt(np.square(df[\"site_longitude\"]) + np.square(df[\"site_latitude\"]))\n",
        "    df[\"phi\"] = np.arctan2(df[\"site_latitude\"], df[\"site_longitude\"])\n",
        "    \n",
        "    # Add rotational cartesian coordinates\n",
        "    df[\"rot_30_x\"] = np.cos(30 * np.pi / 180) * df[\"site_longitude\"] + np.sin(30 * np.pi / 180) * df[\"site_latitude\"]\n",
        "    df[\"rot_30_y\"] = np.sin(30 * np.pi / 180) * df[\"site_longitude\"] - np.cos(30 * np.pi /180) * df[\"site_latitude\"]\n",
        "    \n",
        "    df[\"rot_45_x\"] = np.cos(45 * np.pi / 180) * df[\"site_longitude\"] + np.sin(45 * np.pi / 180) * df[\"site_latitude\"]\n",
        "    df[\"rot_45_y\"] = np.sin(45 * np.pi / 180) * df[\"site_longitude\"] - np.cos(45 * np.pi /180) * df[\"site_latitude\"]\n",
        "    \n",
        "    df[\"rot_60_x\"] = np.cos(60 * np.pi / 180) * df[\"site_longitude\"] + np.sin(60 * np.pi / 180) * df[\"site_latitude\"]\n",
        "    df[\"rot_60_y\"] = np.sin(60 * np.pi / 180) * df[\"site_longitude\"] - np.cos(60 * np.pi /180) * df[\"site_latitude\"]\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:41.089456Z",
          "iopub.status.busy": "2022-10-01T18:22:41.087906Z",
          "iopub.status.idle": "2022-10-01T18:22:41.131852Z",
          "shell.execute_reply": "2022-10-01T18:22:41.130404Z",
          "shell.execute_reply.started": "2022-10-01T18:22:41.089415Z"
        },
        "id": "9m5KQhKk6QLx",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train = extract_geo_spatial_feat(train)\n",
        "test = extract_geo_spatial_feat(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:41.134785Z",
          "iopub.status.busy": "2022-10-01T18:22:41.133607Z",
          "iopub.status.idle": "2022-10-01T18:22:41.256201Z",
          "shell.execute_reply": "2022-10-01T18:22:41.254261Z",
          "shell.execute_reply.started": "2022-10-01T18:22:41.134745Z"
        },
        "id": "ahcorYqF6QLy",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Transform latitude and longitude coordinates using PCA\n",
        "coords = np.vstack((train[[\"site_latitude\", \"site_longitude\"]],\n",
        "                   test[[\"site_latitude\", \"site_longitude\"]]))\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(coords)\n",
        "\n",
        "train[\"pca_0\"] = pca.transform(train[[\"site_latitude\", \"site_longitude\"]])[:,0]\n",
        "train[\"pca_1\"] = pca.transform(train[[\"site_latitude\", \"site_longitude\"]])[:,1]\n",
        "\n",
        "test[\"pca_0\"] = pca.transform(test[[\"site_latitude\", \"site_longitude\"]])[:,0]\n",
        "test[\"pca_1\"] = pca.transform(test[[\"site_latitude\", \"site_longitude\"]])[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:41.404444Z",
          "iopub.status.busy": "2022-10-01T18:22:41.403129Z",
          "iopub.status.idle": "2022-10-01T18:22:41.537749Z",
          "shell.execute_reply": "2022-10-01T18:22:41.536373Z",
          "shell.execute_reply.started": "2022-10-01T18:22:41.404391Z"
        },
        "id": "UGkf93wi6QL2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "feature_counts = pd.concat([train, test]).groupby(\"is_weekend\").size()\n",
        "train[\"weekend_Counts\"] = train[\"is_weekend\"].apply(lambda x: feature_counts[x])\n",
        "test[\"weekend_Counts\"] = test[\"is_weekend\"].apply(lambda x: feature_counts[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:41.541735Z",
          "iopub.status.busy": "2022-10-01T18:22:41.539677Z",
          "iopub.status.idle": "2022-10-01T18:22:41.585365Z",
          "shell.execute_reply": "2022-10-01T18:22:41.584086Z",
          "shell.execute_reply.started": "2022-10-01T18:22:41.541678Z"
        },
        "id": "CemyRFxE6QL2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Binning temperature and humidity\n",
        "est = KBinsDiscretizer(n_bins=5, encode='onehot-dense', strategy='uniform')\n",
        "to_discretize = [\"temp_mean\", \"humidity\"]\n",
        "bin_cols = [f\"temp_mean_bin{bin}\" for bin in range(1, 6)]\n",
        "bin_cols_humidity = [f\"humidity_bin{bin}\" for bin in range(1, 6)]\n",
        "bin_cols.extend(bin_cols_humidity)\n",
        "\n",
        "train[bin_cols] = est.fit_transform(train[to_discretize].fillna(train[to_discretize].mean()))\n",
        "test[bin_cols] = est.transform(test[to_discretize].fillna(test[to_discretize].mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:41.587834Z",
          "iopub.status.busy": "2022-10-01T18:22:41.587382Z",
          "iopub.status.idle": "2022-10-01T18:22:41.595412Z",
          "shell.execute_reply": "2022-10-01T18:22:41.593916Z",
          "shell.execute_reply.started": "2022-10-01T18:22:41.587794Z"
        },
        "id": "WqUC-T0O6QL2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train[\"is_train\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:41.598325Z",
          "iopub.status.busy": "2022-10-01T18:22:41.597449Z",
          "iopub.status.idle": "2022-10-01T18:22:41.639287Z",
          "shell.execute_reply": "2022-10-01T18:22:41.637796Z",
          "shell.execute_reply.started": "2022-10-01T18:22:41.598275Z"
        },
        "id": "QvEAVKJU6QL2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "all_df = pd.concat([train, test])\n",
        "all_df = all_df.sort_values(by=[\"site_latitude\", \"date\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:41.653213Z",
          "iopub.status.busy": "2022-10-01T18:22:41.652208Z",
          "iopub.status.idle": "2022-10-01T18:22:41.676994Z",
          "shell.execute_reply": "2022-10-01T18:22:41.675453Z",
          "shell.execute_reply.started": "2022-10-01T18:22:41.653162Z"
        },
        "id": "O4r_XwrP6QL3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def create_time_series_features(df, lat):\n",
        "    df = df[df.site_latitude == lat]\n",
        "    df[\"humidity_lag1\"] = df[\"humidity\"].shift(1)\n",
        "    df[\"humidity_lag2\"] = df[\"humidity\"].shift(2)\n",
        "    df[\"humidity_lag3\"] = df[\"humidity\"].shift(3)\n",
        "    df[\"humidity_lag4\"] = df[\"humidity\"].shift(4)\n",
        "    df[\"humidity_lag5\"] = df[\"humidity\"].shift(5)\n",
        "    df[\"humidity_lag6\"] = df[\"humidity\"].shift(6)\n",
        "    df[\"humidity_lag7\"] = df[\"humidity\"].shift(7)\n",
        "    \n",
        "    df[\"humidity_lagback1\"] = df[\"humidity\"].shift(-1)\n",
        "    df[\"humidity_lagback2\"] = df[\"humidity\"].shift(-2)\n",
        "    df[\"humidity_lagback3\"] = df[\"humidity\"].shift(-3)\n",
        "    df[\"humidity_lagback4\"] = df[\"humidity\"].shift(-4)\n",
        "    df[\"humidity_lagback5\"] = df[\"humidity\"].shift(-5)\n",
        "    df[\"humidity_lagback6\"] = df[\"humidity\"].shift(-6)\n",
        "    df[\"humidity_lagback7\"] = df[\"humidity\"].shift(-7)\n",
        "    \n",
        "    df[\"temp_lag1\"] = df[\"temp_mean\"].shift(1)\n",
        "    df[\"temp_lag2\"] = df[\"temp_mean\"].shift(2)\n",
        "    df[\"temp_lag3\"] = df[\"temp_mean\"].shift(3)\n",
        "    df[\"temp_lag4\"] = df[\"temp_mean\"].shift(4)\n",
        "    df[\"temp_lag5\"] = df[\"temp_mean\"].shift(5)\n",
        "    df[\"temp_lag6\"] = df[\"temp_mean\"].shift(6)\n",
        "    df[\"temp_lag7\"] = df[\"temp_mean\"].shift(7)\n",
        "    \n",
        "    df[\"temp_lagback1\"] = df[\"temp_mean\"].shift(-1)\n",
        "    df[\"temp_lagback2\"] = df[\"temp_mean\"].shift(-2)\n",
        "    df[\"temp_lagback3\"] = df[\"temp_mean\"].shift(-3)\n",
        "    df[\"temp_lagback4\"] = df[\"temp_mean\"].shift(-4)\n",
        "    df[\"temp_lagback5\"] = df[\"temp_mean\"].shift(-5)\n",
        "    df[\"temp_lagback6\"] = df[\"temp_mean\"].shift(-6)\n",
        "    df[\"temp_lagback7\"] = df[\"temp_mean\"].shift(-7)\n",
        "    \n",
        "    \n",
        "    df[\"humidity_rolling_mean\"] = df[\"humidity\"].rolling(window=3).mean()\n",
        "    df[\"humidity_rolling_max\"] = df[\"humidity\"].rolling(window=3).max()\n",
        "    df[\"humidity_rolling_min\"] = df[\"humidity\"].rolling(window=3).min()\n",
        "    df[\"humidity_rolling_sum\"] = df[\"humidity\"].rolling(window=3).sum()\n",
        "    df[\"humidity_rolling_std\"] = df[\"humidity\"].rolling(window=3).std()\n",
        "\n",
        "    df[\"temp_rolling_mean\"] = df[\"temp_mean\"].rolling(window=3).mean()\n",
        "    df[\"temp_rolling_max\"] = df[\"temp_mean\"].rolling(window=3).max()\n",
        "    df[\"temp_rolling_min\"] = df[\"temp_mean\"].rolling(window=3).min()\n",
        "    df[\"temp_rolling_sum\"] = df[\"temp_mean\"].rolling(window=3).sum()\n",
        "    df[\"temp_rolling_std\"] = df[\"temp_mean\"].rolling(window=3).std()\n",
        "    \n",
        "    df[\"humidity_expanding_mean\"] = df[\"humidity\"].expanding(min_periods=2).mean()\n",
        "    df[\"humidity_expanding_sum\"] = df[\"humidity\"].expanding(min_periods=2).sum()\n",
        "    df[\"humidity_expanding_max\"] = df[\"humidity\"].expanding(min_periods=2).max()\n",
        "    df[\"humidity_expanding_min\"] = df[\"humidity\"].expanding(min_periods=2).min()\n",
        "    df[\"humidity_expanding_std\"] = df[\"humidity\"].expanding(min_periods=2).std()\n",
        "\n",
        "    df[\"temp_expanding_mean\"] = df[\"temp_mean\"].expanding(min_periods=2).mean()\n",
        "    df[\"temp_expanding_sum\"] = df[\"temp_mean\"].expanding(min_periods=2).sum()\n",
        "    df[\"temp_expanding_max\"] = df[\"temp_mean\"].expanding(min_periods=2).max()\n",
        "    df[\"temp_expanding_min\"] = df[\"temp_mean\"].expanding(min_periods=2).min()\n",
        "    df[\"temp_expanding_std\"] = df[\"temp_mean\"].expanding(min_periods=2).std()\n",
        "    \n",
        "    df = df.fillna(0)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:41.679272Z",
          "iopub.status.busy": "2022-10-01T18:22:41.678735Z",
          "iopub.status.idle": "2022-10-01T18:22:41.712020Z",
          "shell.execute_reply": "2022-10-01T18:22:41.710703Z",
          "shell.execute_reply.started": "2022-10-01T18:22:41.679213Z"
        },
        "id": "SINV3ZLq6QL3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def calc_rec_dist(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculates the rectilinear distance between two pointss\n",
        "    :param lat1: latitude of first point\n",
        "    :param lon1: longitude of first point\n",
        "    :param lat2: latitude of second point\n",
        "    :param lon2: longitude of second point\n",
        "    :return: distance\n",
        "    \"\"\"\n",
        "    r = np.sqrt(np.square(lat1 -lat2) + np.square(lon1 - lon2))\n",
        "    return r\n",
        "\n",
        "# Calculate distance between the unique location in test and the locations in train\n",
        "distances = []\n",
        "for lat, lon in zip(all_df[\"site_latitude\"].unique(), all_df[\"site_longitude\"].unique()):\n",
        "    dist = calc_rec_dist(lat, lon, 0.332609275, 32.610047)\n",
        "    distances.append([dist, lat])\n",
        "\n",
        "# get the 3 closest locations\n",
        "distances.sort(key=lambda x: x[0])\n",
        "nearby_lats = np.array(distances[1:4])[:,1]\n",
        "\n",
        "# Let's see how many data points there are at this site\n",
        "unique_location_ids = all_df[all_df.site_latitude == 0.332609275].index\n",
        "len(unique_location_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:41.714613Z",
          "iopub.status.busy": "2022-10-01T18:22:41.714153Z",
          "iopub.status.idle": "2022-10-01T18:22:43.565912Z",
          "shell.execute_reply": "2022-10-01T18:22:43.564581Z",
          "shell.execute_reply.started": "2022-10-01T18:22:41.714572Z"
        },
        "id": "hyue8DwG6QL4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "temp_df = pd.DataFrame()\n",
        "sites = all_df[\"site_latitude\"].unique()\n",
        "for lat in tqdm(sites, total=len(sites)):\n",
        "    if lat in unique_location_ids:\n",
        "        # Calculate the time series features for unique location\n",
        "        # by simulating it being in each of the three cloest sites, then\n",
        "        # averaging the result. You can also try weighted distance\n",
        "        # averaging since the statistics are likely to be most similar\n",
        "        # with the cloest site\n",
        "\n",
        "        group = all_df[all_df.site_latitude == lat]\n",
        "        group1 = create_time_series_features(all_df,nearby_lats[0])\n",
        "        group2 = create_time_series_features(all_df,nearby_lats[1])\n",
        "        group3 = create_time_series_features(all_df,nearby_lats[2])\n",
        "        \n",
        "        cols = group1.columns[-34:]\n",
        "        group[cols] = (group1[cols] + group2[cols] + group3[cols])/3\n",
        "    else:\n",
        "        group = create_time_series_features(all_df, lat)\n",
        "    \n",
        "    temp_df =pd.concat([temp_df, group])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:43.568640Z",
          "iopub.status.busy": "2022-10-01T18:22:43.568132Z",
          "iopub.status.idle": "2022-10-01T18:22:43.594716Z",
          "shell.execute_reply": "2022-10-01T18:22:43.593622Z",
          "shell.execute_reply.started": "2022-10-01T18:22:43.568591Z"
        },
        "id": "7HDmcZKS6QL4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "all_df = temp_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:43.596405Z",
          "iopub.status.busy": "2022-10-01T18:22:43.596060Z",
          "iopub.status.idle": "2022-10-01T18:22:43.620343Z",
          "shell.execute_reply": "2022-10-01T18:22:43.618759Z",
          "shell.execute_reply.started": "2022-10-01T18:22:43.596376Z"
        },
        "id": "DsHy6d856QL4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "all_df[\"humidity_lagback_diff1\"] =  all_df[\"humidity\"] - all_df[\"humidity_lagback1\"]\n",
        "all_df[\"humidity_lagback_diff2\"] =  all_df[\"humidity\"] - all_df[\"humidity_lagback2\"]\n",
        "all_df[\"humidity_lagback_diff3\"] =  all_df[\"humidity\"] - all_df[\"humidity_lagback3\"]\n",
        "all_df[\"humidity_lagback_diff4\"] =  all_df[\"humidity\"] - all_df[\"humidity_lagback4\"]\n",
        "all_df[\"humidity_lagback_diff5\"] =  all_df[\"humidity\"] - all_df[\"humidity_lagback5\"]\n",
        "all_df[\"humidity_lagback_diff6\"] =  all_df[\"humidity\"] - all_df[\"humidity_lagback6\"]\n",
        "all_df[\"humidity_lagback_diff7\"] =  all_df[\"humidity\"] - all_df[\"humidity_lagback7\"]\n",
        "\n",
        "all_df[\"temp_lagback_diff1\"] = all_df[\"temp_mean\"] - all_df[\"temp_lagback1\"]\n",
        "all_df[\"temp_lagback_diff2\"] = all_df[\"temp_mean\"] - all_df[\"temp_lagback2\"]\n",
        "all_df[\"temp_lagback_diff3\"] = all_df[\"temp_mean\"] - all_df[\"temp_lagback3\"]\n",
        "all_df[\"temp_lagback_diff4\"] = all_df[\"temp_mean\"] - all_df[\"temp_lagback4\"]\n",
        "all_df[\"temp_lagback_diff5\"] = all_df[\"temp_mean\"] - all_df[\"temp_lagback5\"]\n",
        "all_df[\"temp_lagback_diff6\"] = all_df[\"temp_mean\"] - all_df[\"temp_lagback6\"]\n",
        "all_df[\"temp_lagback_diff7\"] = all_df[\"temp_mean\"] - all_df[\"temp_lagback7\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:43.623288Z",
          "iopub.status.busy": "2022-10-01T18:22:43.622427Z",
          "iopub.status.idle": "2022-10-01T18:22:43.660782Z",
          "shell.execute_reply": "2022-10-01T18:22:43.659520Z",
          "shell.execute_reply.started": "2022-10-01T18:22:43.623246Z"
        },
        "id": "tb1WmrLB6QL5",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train = all_df[all_df.is_train == 1]\n",
        "test = all_df[all_df.is_train != 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:43.663634Z",
          "iopub.status.busy": "2022-10-01T18:22:43.663012Z",
          "iopub.status.idle": "2022-10-01T18:22:43.680666Z",
          "shell.execute_reply": "2022-10-01T18:22:43.679281Z",
          "shell.execute_reply.started": "2022-10-01T18:22:43.663584Z"
        },
        "id": "u7HZg39X6QL5",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train.drop(columns=[\"is_train\"], inplace=True)\n",
        "test.drop(columns=[\"is_train\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:43.682778Z",
          "iopub.status.busy": "2022-10-01T18:22:43.682301Z",
          "iopub.status.idle": "2022-10-01T18:22:43.694983Z",
          "shell.execute_reply": "2022-10-01T18:22:43.693500Z",
          "shell.execute_reply.started": "2022-10-01T18:22:43.682730Z"
        },
        "id": "h0V0FLAbLo-R",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "len(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtiF7-Mu6QL5",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:43.698284Z",
          "iopub.status.busy": "2022-10-01T18:22:43.697305Z",
          "iopub.status.idle": "2022-10-01T18:22:43.716614Z",
          "shell.execute_reply": "2022-10-01T18:22:43.715287Z",
          "shell.execute_reply.started": "2022-10-01T18:22:43.698233Z"
        },
        "id": "r_jmz86_6QL5",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Target Encoding\n",
        "class TargetEncode(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, categories='auto', k=1, f=1, noise_level=0, random_state=None):\n",
        "        if type(categories)==str and categories!='auto':\n",
        "            self.categories = [categories]\n",
        "        else:\n",
        "            self.categories = categories\n",
        "        self.k = k\n",
        "        self.f = f\n",
        "        self.noise_level = noise_level\n",
        "        self.encodings = dict()\n",
        "        self.prior = None\n",
        "        self.random_state = random_state\n",
        "        \n",
        "    def add_noise(self, series, noise_level):\n",
        "        return series * (1 + noise_level * np.random.randn(len(series)))\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        if type(self.categories)=='auto':\n",
        "            self.categories = np.where(X.dtypes == type(object()))[0]\n",
        "            \n",
        "        temp = X.loc[:, self.categories].copy()\n",
        "        temp['Label'] = y\n",
        "        self.prior = np.mean(y)\n",
        "        for variable in self.categories:\n",
        "            avg = (temp.groupby(by=variable)['Label']\n",
        "                   .agg(['mean', 'count']))\n",
        "            # Compute smoothing\n",
        "            smoothing = (1 / (1 + np.exp(-(avg['count'] - self.k) /\n",
        "                                         self.f)))\n",
        "            # The bigger the count the less full_avg is accounted\n",
        "            self.encodings[variable] = dict(self.prior * (1 -\n",
        "                                                          smoothing) + avg['mean'] * smoothing)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Xt = X.copy()\n",
        "        for variable in self.categories:\n",
        "            Xt[variable].replace(self.encodings[variable], inplace=True)\n",
        "            unknown_value = {value:self.prior for value in \n",
        "                             X[variable].unique() \n",
        "                             if value not in\n",
        "                             self.encodings[variable].keys()}\n",
        "            if len(unknown_value) > 0:\n",
        "                Xt[variable].replace(unknown_value, inplace=True)\n",
        "            Xt[variable] = Xt[variable].astype(float)\n",
        "            if self.noise_level > 0:\n",
        "                if self.random_state is not None:\n",
        "                    np.random.seed(self.random_state)\n",
        "                Xt[variable] = self.add_noise(Xt[variable], self.noise_level)\n",
        "        return Xt\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        self.fit(X, y)\n",
        "        return self.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:43.845478Z",
          "iopub.status.busy": "2022-10-01T18:22:43.845000Z",
          "iopub.status.idle": "2022-10-01T18:22:43.855092Z",
          "shell.execute_reply": "2022-10-01T18:22:43.853532Z",
          "shell.execute_reply.started": "2022-10-01T18:22:43.845433Z"
        },
        "id": "J3ID1c0i6QL6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# First duplication this column, we'll need it later\n",
        "train[\"station\"] = train[\"device\"]\n",
        "test[\"station\"] = test[\"device\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:43.857173Z",
          "iopub.status.busy": "2022-10-01T18:22:43.856653Z",
          "iopub.status.idle": "2022-10-01T18:22:43.930221Z",
          "shell.execute_reply": "2022-10-01T18:22:43.929135Z",
          "shell.execute_reply.started": "2022-10-01T18:22:43.857126Z"
        },
        "id": "t8ehvhy_6QL6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Target encode device column\n",
        "te = TargetEncode(categories=[\"device\"])\n",
        "te.fit(train.drop(columns=[\"pm2_5\"]), train['pm2_5'])\n",
        "train = te.transform(train)\n",
        "test = te.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUSAguBsIRnR",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "nearby_lats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:43.932240Z",
          "iopub.status.busy": "2022-10-01T18:22:43.931774Z",
          "iopub.status.idle": "2022-10-01T18:22:43.963897Z",
          "shell.execute_reply": "2022-10-01T18:22:43.962544Z",
          "shell.execute_reply.started": "2022-10-01T18:22:43.932207Z"
        },
        "id": "pl4e0D976QL6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# for the unique location let's find the average for 3 nearby locations\n",
        "loc1 = train[train.site_latitude == nearby_lats[0]][\"device\"]\n",
        "loc2 = train[train.site_latitude == nearby_lats[1]][\"device\"]\n",
        "loc3 = train[train.site_latitude == nearby_lats[2]][\"device\"]\n",
        "\n",
        "avg = (loc1.iloc[0] + loc2.iloc[0] + loc3.iloc[0]) / 3\n",
        "\n",
        "test = test.reset_index().drop(columns=[\"index\"])\n",
        "idx = test[test.site_latitude == 0.332609275].index\n",
        "test[\"device\"].iloc[idx] = avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:22:43.965993Z",
          "iopub.status.busy": "2022-10-01T18:22:43.965504Z",
          "iopub.status.idle": "2022-10-01T18:22:43.978876Z",
          "shell.execute_reply": "2022-10-01T18:22:43.977859Z",
          "shell.execute_reply.started": "2022-10-01T18:22:43.965944Z"
        },
        "id": "sudGEtua-0b8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train = train.set_index(\"ID\")\n",
        "test = test.set_index(\"ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlp-5oGR7a1i",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T18:23:10.868314Z",
          "iopub.status.busy": "2022-10-01T18:23:10.867839Z",
          "iopub.status.idle": "2022-10-01T18:23:10.883244Z",
          "shell.execute_reply": "2022-10-01T18:23:10.882064Z",
          "shell.execute_reply.started": "2022-10-01T18:23:10.868272Z"
        },
        "id": "dS-qt_M9Lo-Y",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "STATIC_REAL = [\"station\", \"device\", \"pca_0\",\"pca_1\", \"site_latitude\", \"site_longitude\", \"polar_radius\", \"phi\", \"rot_30_x\", \n",
        "               \"rot_30_y\", \"rot_45_x\", \"rot_45_y\", \"rot_60_x\",\"rot_60_y\", 'weekend_Counts',\n",
        "              'month_sin', 'month_cos', 'weekday_sin', 'weekday_cos', 'cyclic_month_spline_1', 'cyclic_month_spline_2',\n",
        "               'cyclic_month_spline_3', 'cyclic_month_spline_4', 'cyclic_month_spline_5',\n",
        "              'cyclic_month_spline_6', 'cyclic_weekday_spline_1', 'cyclic_weekday_spline_2']\n",
        "\n",
        "DYNAMIC_CAT = ['year', 'month', 'day', 'weekday', 'quarter', 'is_month_start', 'is_month_end', 'is_quarter_start', \n",
        "               'is_quarter_end', 'is_weekend']\n",
        "\n",
        "DYNAMIC_REAL = [\"time_idx\", \"humidity\", \"temp_mean\", 'SulphurDioxide_SO2_column_number_density',\n",
        "                'SulphurDioxide_SO2_column_number_density_amf', 'SulphurDioxide_SO2_slant_column_number_density', \n",
        "                'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', 'SulphurDioxide_sensor_zenith_angle',\n",
        "                'SulphurDioxide_solar_azimuth_angle', 'SulphurDioxide_solar_zenith_angle', 'SulphurDioxide_SO2_column_number_density_15km',\n",
        "                'CarbonMonoxide_CO_column_number_density', 'CarbonMonoxide_H2O_column_number_density', 'CarbonMonoxide_cloud_height',\n",
        "                'CarbonMonoxide_sensor_altitude', 'CarbonMonoxide_sensor_azimuth_angle', 'CarbonMonoxide_sensor_zenith_angle',\n",
        "                'CarbonMonoxide_solar_azimuth_angle', 'CarbonMonoxide_solar_zenith_angle', 'NitrogenDioxide_NO2_column_number_density',\n",
        "                'NitrogenDioxide_tropospheric_NO2_column_number_density', 'NitrogenDioxide_stratospheric_NO2_column_number_density',\n",
        "                'NitrogenDioxide_NO2_slant_column_number_density', 'NitrogenDioxide_tropopause_pressure', 'NitrogenDioxide_absorbing_aerosol_index',\n",
        "                'NitrogenDioxide_cloud_fraction', 'NitrogenDioxide_sensor_altitude', 'NitrogenDioxide_sensor_azimuth_angle',\n",
        "                'NitrogenDioxide_sensor_zenith_angle', 'NitrogenDioxide_solar_azimuth_angle', 'NitrogenDioxide_solar_zenith_angle',\n",
        "                'Formaldehyde_tropospheric_HCHO_column_number_density', 'Formaldehyde_tropospheric_HCHO_column_number_density_amf',\n",
        "                'Formaldehyde_HCHO_slant_column_number_density', 'Formaldehyde_cloud_fraction', 'Formaldehyde_solar_zenith_angle',\n",
        "                'Formaldehyde_solar_azimuth_angle', 'Formaldehyde_sensor_zenith_angle', 'Formaldehyde_sensor_azimuth_angle',\n",
        "                'UvAerosolIndex_absorbing_aerosol_index', 'UvAerosolIndex_sensor_altitude', 'UvAerosolIndex_sensor_azimuth_angle', \n",
        "                'UvAerosolIndex_sensor_zenith_angle', 'UvAerosolIndex_solar_azimuth_angle', 'UvAerosolIndex_solar_zenith_angle',\n",
        "                 'Ozone_O3_column_number_density', 'Ozone_O3_column_number_density_amf', 'Ozone_O3_slant_column_number_density',\n",
        "                 'Ozone_O3_effective_temperature', 'Ozone_cloud_fraction', 'Ozone_sensor_azimuth_angle', 'Ozone_sensor_zenith_angle',\n",
        "                 'Ozone_solar_azimuth_angle', 'Ozone_solar_zenith_angle', 'Cloud_cloud_fraction', 'Cloud_cloud_top_pressure', 'Cloud_cloud_top_height',\n",
        "                 'Cloud_cloud_base_pressure', 'Cloud_cloud_base_height', 'Cloud_cloud_optical_depth', 'Cloud_surface_albedo', 'Cloud_sensor_azimuth_angle',\n",
        "                 'Cloud_sensor_zenith_angle', 'Cloud_solar_azimuth_angle', 'Cloud_solar_zenith_angle']\n",
        "\n",
        "DYNAMIC_REAL_UNKNOWN = [\"pm2_5\", 'temp_mean_bin1', 'temp_mean_bin2', 'temp_mean_bin3', 'temp_mean_bin4', 'temp_mean_bin5',\n",
        "                         'humidity_bin1', 'humidity_bin2', 'humidity_bin3', 'humidity_bin4', 'humidity_bin5', 'humidity_lag1',\n",
        "                        'humidity_lag2', 'humidity_lag3', 'humidity_lag4', 'humidity_lag5', 'humidity_lag6', 'humidity_lag7',\n",
        "                        'humidity_lagback1', 'humidity_lagback2', 'humidity_lagback3', 'humidity_lagback4', 'humidity_lagback5',\n",
        "                        'humidity_lagback6', 'humidity_lagback7', 'temp_lag1', 'temp_lag2', 'temp_lag3', 'temp_lag4', 'temp_lag5',\n",
        "                        'temp_lag6', 'temp_lag7', 'temp_lagback1', 'temp_lagback2', 'temp_lagback3', 'temp_lagback4',\n",
        "                        'temp_lagback5', 'temp_lagback6', 'temp_lagback7', 'humidity_rolling_mean', 'humidity_rolling_max',\n",
        "                        'humidity_rolling_min', 'humidity_rolling_sum', 'humidity_rolling_std', 'temp_rolling_mean', 'temp_rolling_max', \n",
        "                        'temp_rolling_min', 'temp_rolling_sum', 'temp_rolling_std','humidity_expanding_mean', 'humidity_expanding_sum',\n",
        "                        'humidity_expanding_max', 'humidity_expanding_min', 'humidity_expanding_std', 'temp_expanding_mean',\n",
        "                        'temp_expanding_sum','temp_expanding_max', 'temp_expanding_min','temp_expanding_std', 'humidity_lagback_diff1', \n",
        "                        'humidity_lagback_diff2','humidity_lagback_diff3','humidity_lagback_diff4','humidity_lagback_diff5',\n",
        "                        'humidity_lagback_diff6','humidity_lagback_diff7','temp_lagback_diff1','temp_lagback_diff2','temp_lagback_diff3',\n",
        "                        'temp_lagback_diff4','temp_lagback_diff5','temp_lagback_diff6','temp_lagback_diff7']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "63jo_ealuPfC"
      },
      "outputs": [],
      "source": [
        "class DataPipeline():\n",
        "    \"\"\"\n",
        "    This class contains utilities for creating a time_index, adding\n",
        "    # pseudo labels, and transforming datatypes and reducing memory usage\n",
        "    objects.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, df_train: pd.DataFrame, df_test: pd.DataFrame, add_pseudo_labels=False):\n",
        "#         self.cut_off = valid_cutoff\n",
        "        self.train = df_train\n",
        "        self.test = df_test\n",
        "        self.pseudo_label = add_pseudo_labels\n",
        "        \n",
        "        \n",
        "    def transform(self):\n",
        "        df = pd.concat([self.train, self.test])\n",
        "        \n",
        "        # Add time index\n",
        "        df['time_idx'] = (df['date'].dt.date - df['date'].dt.date.min()).dt.days\n",
        "        self.train[\"time_idx\"] = df[\"time_idx\"].iloc[:len(self.train)]\n",
        "        self.test[\"time_idx\"] = df[\"time_idx\"].iloc[len(self.train):]\n",
        "        \n",
        "        # Convert categorical column to numerical form to use it as a group id\n",
        "        le = LabelEncoder()\n",
        "        le.fit(df[\"station\"])\n",
        "        \n",
        "        self.train[\"station\"] = le.transform(self.train[\"station\"])\n",
        "        self.test[\"station\"] = le.transform(self.test[\"station\"])\n",
        "\n",
        "        if self.pseudo_label:\n",
        "            # Add some pseudo labels\n",
        "            pseudo_samples = self.test[self.test[\"date\"] <= \"2020-09-20\"]\n",
        "            pseudo_labels = layer.get_dataset(\"nkosana_mlandu/zindi-air-quality-prediction-challenge/datasets/pseudo_labels:1.1\").to_pandas().values\n",
        "            pseudo_samples[\"pm2_5\"] = pseudo_labels\n",
        "\n",
        "            # Filter pseudo samples to remove rare stations\n",
        "            # Station 34 and 35 are not in the train set\n",
        "            pseudo_samples = pseudo_samples[pseudo_samples[\"station\"] < 34]\n",
        "        \n",
        "        self.train = pd.concat([self.train, pseudo_samples])\n",
        "        \n",
        "        # Pytorch expects all categorical columns to be \"str\"\n",
        "        self.train[DYNAMIC_CAT] = self.train[DYNAMIC_CAT].astype(\"str\")\n",
        "        self.test[DYNAMIC_CAT] = self.test[DYNAMIC_CAT].astype(\"str\")\n",
        "\n",
        "        self.train = reduce_mem_usage(self.train)\n",
        "        self.test = reduce_mem_usage(self.test)\n",
        "        return self.train, self.test\n",
        "\n",
        "\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64',\n",
        "                'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7_U_cJNAuPfF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# A Note on pseudo labeling: We only added up to 1 month into the test set per \n",
        "# each time series, and all pseudo labelled examples fall into\n",
        "# range > max_prediction length thus will be added to the validation set.\n",
        "# There's no risk of contaminating the train set, but however \n",
        "# on the downside this impacts the evaluation metric\n",
        "pipe = DataPipeline(train, test, add_pseudo_labels=True)\n",
        "train, test = pipe.transform()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"time_idx\"].max()"
      ],
      "metadata": {
        "id": "uIfTOGJeyBcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T19:42:03.082823Z",
          "iopub.status.busy": "2022-10-01T19:42:03.082327Z",
          "iopub.status.idle": "2022-10-01T19:42:04.587694Z",
          "shell.execute_reply": "2022-10-01T19:42:04.586465Z",
          "shell.execute_reply.started": "2022-10-01T19:42:03.082780Z"
        },
        "id": "tt9OZMux7a1j",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "max_prediction_length = 134 # predict full length of test set\n",
        "max_encoder_length = train[\"date\"].nunique()\n",
        "\n",
        "training = TimeSeriesDataSet(\n",
        "    train,\n",
        "    time_idx=\"time_idx\",\n",
        "    target=\"pm2_5\",\n",
        "    group_ids=[\"station\"],\n",
        "    min_encoder_length=0,\n",
        "    max_encoder_length=max_encoder_length,\n",
        "    min_prediction_length=1,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "    static_reals=STATIC_REAL,\n",
        "    time_varying_known_reals=DYNAMIC_REAL,\n",
        "    time_varying_known_categoricals=DYNAMIC_CAT,\n",
        "    time_varying_unknown_categoricals=[],\n",
        "    time_varying_unknown_reals=DYNAMIC_REAL_UNKNOWN,\n",
        "    categorical_encoders={\n",
        "       'month': pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
        "        'day': pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True)   \n",
        "    },\n",
        "#     target_normalizer=GroupNormalizer(\n",
        "#         groups=[\"station\"], transformation=\"softplus\"\n",
        "#     ),  # use softplus and normalize by group\n",
        "    target_normalizer=GroupNormalizer(\n",
        "        groups=[\"station\"], transformation=\"log\"\n",
        "    ), # use log transfooooration and normalize by group\n",
        "    scalers={\"StandardScaler\": StandardScaler()},\n",
        "    add_relative_time_idx=False,\n",
        "    add_target_scales=False,\n",
        "    add_encoder_length=False,\n",
        "    allow_missing_timesteps=True,\n",
        ")\n",
        "\n",
        "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
        "# for each series\n",
        "validation = TimeSeriesDataSet.from_dataset(training, train, predict=True, stop_randomization=True)\n",
        "\n",
        "# create dataloaders for model\n",
        "batch_size = 128\n",
        "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
        "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T19:44:26.181035Z",
          "iopub.status.busy": "2022-10-01T19:44:26.180565Z",
          "iopub.status.idle": "2022-10-01T19:44:26.235809Z",
          "shell.execute_reply": "2022-10-01T19:44:26.234415Z",
          "shell.execute_reply.started": "2022-10-01T19:44:26.181000Z"
        },
        "id": "QbFhasiU7a1k",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
        "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
        "baseline_predictions = Baseline().predict(val_dataloader)\n",
        "(actuals - baseline_predictions).abs().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE6jmTgTEI-9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "actuals.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-01T19:49:34.123552Z",
          "iopub.status.idle": "2022-10-01T19:49:34.124004Z",
          "shell.execute_reply": "2022-10-01T19:49:34.123809Z",
          "shell.execute_reply.started": "2022-10-01T19:49:34.123788Z"
        },
        "id": "J0iF-tPe7a1k",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# configure network and trainer\n",
        "pl.seed_everything(42)\n",
        "trainer = pl.Trainer(     \n",
        "    accelerator=\"gpu\",\n",
        "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
        "    # of the gradient for recurrent neural networks\n",
        "    gradient_clip_val=0.1,\n",
        ")\n",
        "\n",
        "\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    # not meaningful for finding the learning rate but otherwise very important\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
        "    # number of attention heads. Set to up to 4 for large datasets\n",
        "    attention_head_size=1,\n",
        "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
        "    hidden_continuous_size=8,  # set to <= hidden_size\n",
        "    output_size=1,  # 7 quantiles by default\n",
        "    loss=SMAPE(),\n",
        "    # reduce learning rate if no improvement in validation loss after x epochs\n",
        "    reduce_on_plateau_patience=4,\n",
        ")\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning Hyperparameters"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "A-kHBNXGuPfQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "k5nQmLSBuPfS"
      },
      "outputs": [],
      "source": [
        "# SKIP\n",
        "#study = optimize_hyperparameters(\n",
        "#    train_dataloader,\n",
        "#    val_dataloader,\n",
        "#    model_path=\"optuna_test\",\n",
        "#    n_trials=200, \n",
        "#    max_epochs=50,\n",
        "#    gradient_clip_val_range=(0.01, 1.0),\n",
        "#    hidden_size_range=(8, 128),\n",
        "#    hidden_continuous_size_range=(8, 128),\n",
        "#    attention_head_size_range=(1, 4),\n",
        "#    learning_rate_range=(0.001, 0.1),\n",
        "#    dropout_range=(0.1, 0.3),\n",
        "#    trainer_kwargs=dict(limit_train_batches=30),\n",
        "#    reduce_on_plateau_patience=4,\n",
        "#    use_learning_rate_finder=False,\n",
        "#)\n",
        "#with open(\"test_study.pkl\", \"wb\") as fout:\n",
        "#    pickle.dump(study, fout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-01T19:49:18.561130Z",
          "iopub.status.busy": "2022-10-01T19:49:18.560614Z",
          "iopub.status.idle": "2022-10-01T19:49:34.122572Z",
          "shell.execute_reply": "2022-10-01T19:49:34.119971Z",
          "shell.execute_reply.started": "2022-10-01T19:49:18.561061Z"
        },
        "id": "JXT6jVe8i0N-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#  find optimal learning rate\n",
        "res = trainer.tuner.lr_find(\n",
        "     tft,\n",
        "     train_dataloaders=train_dataloader,\n",
        "     val_dataloaders=val_dataloader,\n",
        "     max_lr=10,\n",
        "     min_lr=1e-9)\n",
        "\n",
        "print(f\"suggested learning rate: {res.suggestion()}\")\n",
        "fig = res.plot(show=True, suggest=True)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OCsws1aMuPfW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "795aR8o3jg6D",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9364abnuVpv9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "MAX_EPOCHS = 30\n",
        "GRADIENT_CLIP_VAL = 0.1\n",
        "LEARNING_RATE = 0.6\n",
        "HIDDEN_SIZE = 16\n",
        "ATTENTION_HEAD_SIZE = 1\n",
        "DROPOUT = 0.2\n",
        "HIDDEN_CONTINOUS_SIZE = 8\n",
        "OUTPUT_SIZE = 1\n",
        "LOSS = SMAPE()\n",
        "\n",
        "# configure network and trainer\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
        "lr_logger = LearningRateMonitor()  # log the learning rate\n",
        "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
        "#logger = LayerLogger(project_name='zindi-air-quality-prediction-challenge', api_key='[API-KEY]')\n",
        "\n",
        "    \n",
        "trainer = pl.Trainer(\n",
        "        max_epochs=MAX_EPOCHS,\n",
        "        accelerator=\"gpu\",\n",
        "        enable_model_summary=True,\n",
        "        gradient_clip_val=GRADIENT_CLIP_VAL,         \n",
        "        #limit_train_batches=30, # comment in for training, running valiation every 30 batches\n",
        "        # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
        "        callbacks=[lr_logger, early_stop_callback],\n",
        "        logger=logger)\n",
        "\n",
        "@model(\"TFT-beta\")\n",
        "def train_model():\n",
        "    \n",
        "\n",
        "    model = TemporalFusionTransformer.from_dataset(\n",
        "        training,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "#         lstm_layers=LSTM_LAYERS,\n",
        "        hidden_size=HIDDEN_SIZE,  \n",
        "        attention_head_size=ATTENTION_HEAD_SIZE,\n",
        "        dropout=DROPOUT,          \n",
        "        hidden_continuous_size=HIDDEN_CONTINOUS_SIZE,  \n",
        "        output_size=OUTPUT_SIZE,\n",
        "        loss=LOSS,\n",
        "        reduce_on_plateau_patience=4,\n",
        "    )\n",
        "    \n",
        "    params = {\"learning_rate\": LEARNING_RATE,\n",
        "#               \"lstm_layers\": LSTM_LAYERS,\n",
        "              \"hidden_size\": HIDDEN_SIZE,  \n",
        "              \"attention_head_size\": ATTENTION_HEAD_SIZE,\n",
        "              \"dropout\": DROPOUT,          \n",
        "              \"hidden_continuous_size\": HIDDEN_CONTINOUS_SIZE,  \n",
        "              \"output_size\": OUTPUT_SIZE,\n",
        "              \"loss\": 'SMAPE'}\n",
        "    layer.log(params)  # Log parameters\n",
        "\n",
        "    print(f\"Number of parameters in network: {model.size()/1e3:.1f}k\")     \n",
        "    layer.log({\"Number of parameters in network\": f\"{model.size()/1e3:.1f}k\"})\n",
        "    \n",
        "    # fit network\n",
        "    trainer.fit(\n",
        "        model,\n",
        "        train_dataloaders=train_dataloader,\n",
        "        val_dataloaders=val_dataloader)\n",
        "    \n",
        "    \n",
        "    # load the best model according to the validation loss\n",
        "    # (given that we use early stopping, this is not necessarily the last epoch)\n",
        "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "    best_model = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
        "    \n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Each epoch takes around 20 mins on the complete dataset.\n",
        "# Thus running up to MAX_EPOCHS will take approx. 10 hours"
      ],
      "metadata": {
        "id": "UWN825TRYU7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh2iD4zXVpwB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "best_model = train_model()\n",
        "# layer.run([train_model], debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wDNylKJ6uPfy"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard.\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eN92yX90rQ5",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4RWUMhJVpwL",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "@model(\"TFT-evaluation\")\n",
        "def evaluate_model():\n",
        "    # load the best model according to the validation loss\n",
        "    # (given that we use early stopping, this is not necessarily the last epoch)\n",
        "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "    best_model = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
        "    \n",
        "    # calcualte mean absolute error on validation set\n",
        "    actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
        "    predictions = best_model.predict(val_dataloader)\n",
        "    valid_mae = (actuals - predictions).abs().mean()\n",
        "    \n",
        "    print(f\"Valid MAE: {valid_mae}\")\n",
        "    layer.log({\"Validation MAE\": f\"{valid_mae}\"})\n",
        "    \n",
        "    sm = SMAPE()\n",
        "    print(f\"Validation median SMAPE loss: {sm.loss(actuals, predictions).mean(axis = 1).median().item()}\")\n",
        "    layer.log({\"Validation median SMAPE loss\": f\"{sm.loss(actuals, predictions).mean(axis = 1).median().item()}\"})\n",
        "    \n",
        "    \n",
        "    # raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
        "    raw_predictions, x = best_model.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
        "    for idx in range(34):  # plot 34 examples\n",
        "        fig = best_model.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)\n",
        "        layer.log({\"Plot of predictions\":fig}, step=idx)\n",
        "    \n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm4Sas8R00LQ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "best_model = evaluate_model()\n",
        "# layer.run([evaluate_model])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "456XVaM21Qlx",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvMzEDwLVpwR",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "@model(\"TFT-error-_analysis\")\n",
        "def model_error_analysis():\n",
        "    # raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
        "    raw_predictions, x = best_model.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
        "\n",
        "    # calcualte metric by which to display\n",
        "    predictions = best_model.predict(val_dataloader)\n",
        "    mean_losses = SMAPE(reduction=\"none\")(predictions, actuals).mean(1)\n",
        "    indices = mean_losses.argsort(descending=True)  # sort losses\n",
        "    \n",
        "    for idx in range(10):  # plot 10 examples\n",
        "         fig = best_model.plot_prediction(\n",
        "            x, raw_predictions, idx=indices[idx], add_loss_to_title=SMAPE(quantiles=best_model.loss.quantiles))\n",
        "         layer.log({f\"Worst performers\": fig}, step=idx)\n",
        "    predictions, x = best_model.predict(val_dataloader, return_x=True)        \n",
        "    predictions_vs_actuals = best_model.calculate_prediction_actual_by_variable(x, predictions)\n",
        "    fig_2 = best_model.plot_prediction_actual_by_variable(predictions_vs_actuals);\n",
        "    layer.log({\"Predicted vs Actual\": fig_2})\n",
        "    \n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4zIfaXR1bM_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "_ = model_error_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QKt6us6a105",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uHVv1tqtuPf-"
      },
      "outputs": [],
      "source": [
        "# load the best model according to the validation loss\n",
        "# (given that we use early stopping, this is not necessarily the last epoch)\n",
        "best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "best_model = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rgdXlWdWqos",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# unique devices = aq_91 and aq_98, latitude = 0.332609275\n",
        "# Ideas: replace device with device from 3 nearest locations, then average the predictions. Also try weighted averaging based on distance to location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qU9KjcBLqbn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Replace aq_91 and aq_98 with APYZC5J7, aq_36, aq_74"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czx1Q5eyVpwZ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#@dataset(\"generate_predictions\")\n",
        "def generate_predictions(df, plot=False):\n",
        "    # select last 557 days from data (max_encoder_length is 557)\n",
        "    encoder_data = train[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
        "    #cutoff = \"2020-2-20\"\n",
        "    #end_train = \"2020-08-20\"\n",
        "    #encoder_data = train[lambda x: (x.date >= cutoff) & (x.date < end_train)]    \n",
        "    \n",
        "    decoder_data = df.copy()\n",
        "    \n",
        "    # combine encoder and decoder data\n",
        "    new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)\n",
        "    \n",
        "    predictions = best_model.predict(new_prediction_data, return_x=False)\n",
        "    \n",
        "    if plot:\n",
        "        new_raw_predictions, new_x = best_model.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
        "        for idx in range(34):  # plot 34 examples\n",
        "            fig = best_model.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False)\n",
        "            \n",
        "            #layer.log({\"Inference\": fig}, step=idx)\n",
        "        \n",
        "        interpretation = best_model.interpret_output(new_raw_predictions, reduction=\"sum\")\n",
        "        fig_2 = best_model.plot_interpretation(interpretation)\n",
        "        #layer.log({\"Model intepretation\": fig_2})\n",
        "    \n",
        "\n",
        "    predictions_df = pd.DataFrame(predictions.numpy()).T\n",
        "    predictions_df['time_idx'] = sorted(df['time_idx'].unique())\n",
        "\n",
        "    predictions_df2 = pd.melt(predictions_df, id_vars=['time_idx'])\n",
        "    predictions_df2.rename(columns = {'value':'pm2_5', 'variable':'station'}, inplace = True)\n",
        "\n",
        "    #now copying the results back to df_test as per location and time\n",
        "    result = pd.merge(df, predictions_df2, on=[\"time_idx\", \"station\"])\n",
        "    result.rename(columns={'pm2_5_y': \"pm2_5\"}, inplace=True)\n",
        "    result[\"ID\"] = df.index\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR7U_5nuVpwb",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def generate_predictions_unique_stations(strategy=\"mean\"):\n",
        "    test_pseudo_1 = test.copy()\n",
        "    test_pseudo_1[\"station\"] = test_pseudo_1[\"station\"].replace({34:15, 35: 15})\n",
        "    pred_1 = generate_predictions(test_pseudo_1)\n",
        "    pred_1 = pred_1[pred_1.site_latitude == 0.332609275]\n",
        "    \n",
        "    test_pseudo_2 = test.copy()\n",
        "    test_pseudo_2[\"station\"] = test_pseudo_2[\"station\"].replace({34:31, 35: 31})\n",
        "    pred_2 = generate_predictions(test_pseudo_2)\n",
        "    pred_2 = pred_2[pred_2.site_latitude == 0.332609275]\n",
        "    \n",
        "    test_pseudo_3 = test.copy()\n",
        "    test_pseudo_3[\"station\"] = test_pseudo_3[\"station\"].replace({34:6, 35: 6})\n",
        "    pred_3 = generate_predictions(test_pseudo_3)\n",
        "    pred_3 = pred_3[pred_3.site_latitude == 0.332609275]\n",
        "    \n",
        "    \n",
        "    if strategy ==\"mean\":\n",
        "        preds = np.mean([pred_1[\"pm2_5\"].values, pred_2[\"pm2_5\"].values, pred_3[\"pm2_5\"]], axis=0)\n",
        "    elif strategy == \"median\":\n",
        "        preds = np.median([pred_1[\"pm2_5\"].values, pred_2[\"pm2_5\"].values, pred_3[\"pm2_5\"]], axis=0)\n",
        "\n",
        "    elif strategy == \"mean_weighted\":\n",
        "        distances_to_station = np.array(distances[1:4])[:,0]    \n",
        "        weights = sorted([dist / np.sum(distances_to_station) for dist in distances_to_station], \n",
        "                        reverse=True) \n",
        "    \n",
        "        preds = np.average([pred_1[\"pm2_5\"].values, pred_2[\"pm2_5\"].values, pred_3[\"pm2_5\"]], \n",
        "                             weights=weights, axis=0)\n",
        "        \n",
        "        \n",
        "    df = test[test.site_latitude == 0.332609275]\n",
        "    df[\"pm2_5\"] = preds\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPk-9IRFVpwe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# test['time_idx'] = (test['date'].dt.date - train['date'].dt.date.min()).dt.days\n",
        "test_clean = test[~test.station.isin([34, 35])]\n",
        "\n",
        "known_stations = generate_predictions(test_clean,plot=False)\n",
        "new_stations = generate_predictions_unique_stations(strategy=\"mean_weighted\")\n",
        "new_stations = new_stations.reset_index()\n",
        "\n",
        "test_preds = pd.concat([known_stations, new_stations])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ntyje6ID88ti",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "test_preds[[\"ID\", \"station\", \"site_latitude\", \"site_longitude\", \"pm2_5\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JboecLmo6QMA",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71K-3tqN6QMB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "test_preds[[\"ID\", \"pm2_5\"]].to_csv(\"best_submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acknowledgements and concluding remarks\n",
        "We acknowledge the competition organisers [Layer](https://layer.ai), the dataset providers [AirQo](https://platform.airqo.net) and to our beloved friends at [Zindi](https://zindi.africa) thank you for coninuing to advance the cause of Data Science in Africa. To our fellow data scientists we hope you had fun, and most importantly you learnt a lot through the competition and that you'll continue advancing your skills. Let's keep building data solutions for Africa, Zindisha!"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7ejVxPiiuPgO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Kd6rJdYhuPgP"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}