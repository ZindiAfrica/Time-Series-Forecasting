{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm_notebook\n",
    "import geopandas as gpd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = './input_files/'\n",
    "OUTPUT_PATH = './output_files/model_1_partial_terra.csv'\n",
    "df = pd.read_csv(DATA_FOLDER + \"Train.csv\")\n",
    "sub_df = pd.read_csv(DATA_FOLDER + \"SampleSubmission.csv\")\n",
    "wetlands = pd.read_csv(DATA_FOLDER + 'wetlands.csv')\n",
    "TerraClimateDataTrain = pd.read_csv('../ExternalData/TerraClimateDataTrain.csv')#https://developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_TERRACLIMATE\n",
    "TerraClimateDataTest = pd.read_csv('../ExternalData/TerraClimateDataTest.csv')#https://developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_TERRACLIMATE\n",
    "TerraClimateDataTrain = TerraClimateDataTrain[['vap1yearbefore_month_2','vap2yearsbefore_month_1','vap2yearsbefore_month_3','vap2yearsbefore_month_4','vap2yearsbefore_month_6','vap1yearbefore_month_1','vap1yearbefore_month_5','X','Y']]\n",
    "TerraClimateDataTest  = TerraClimateDataTest[['vap1yearbefore_month_2','vap2yearsbefore_month_1','vap2yearsbefore_month_3','vap2yearsbefore_month_4','vap2yearsbefore_month_6','vap1yearbefore_month_1','vap1yearbefore_month_5','X','Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precip_cols = [c for c in df.columns if 'precip' in c]\n",
    "len(precip_cols), len(precip_cols)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_cols_2015 = precip_cols[0: 17]\n",
    "precip_cols_2019 = precip_cols[17: 34]\n",
    "train = df[precip_cols_2015]\n",
    "train.columns = [f'precip_week_{i}' for i in range(1, 18)]\n",
    "test = df[precip_cols_2019]\n",
    "test.columns = [f'precip_week_{i}' for i in range(1, 18)]\n",
    "train = pd.concat([train, df[['X', 'Y', 'target_2015', 'elevation', 'LC_Type1_mode', 'Square_ID']]], axis=1)\n",
    "test = pd.concat([test, df[['X', 'Y', 'target_2015', 'elevation', 'LC_Type1_mode', 'Square_ID']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(TerraClimateDataTrain,how=\"left\",on=['X','Y'])\n",
    "test = test.merge(TerraClimateDataTest,how=\"left\",on=['X','Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.X, df.Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32932, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train, test]).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['precip_week_1', 'precip_week_2', 'precip_week_3', 'precip_week_4', 'precip_week_5', 'precip_week_6', 'precip_week_7', 'precip_week_8', 'precip_week_9', 'precip_week_10', 'precip_week_11', 'precip_week_12', 'precip_week_13', 'precip_week_14', 'precip_week_15', 'precip_week_16', 'precip_week_17']\n"
     ]
    }
   ],
   "source": [
    "precip_cols = [c for c in df.columns if 'precip' in c]\n",
    "len(precip_cols)\n",
    "print(precip_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_diff_interacts(df, inter_cols):\n",
    "    '''\n",
    "    This function calucates all possible combinations of differences for columns provided to into pairs for the given dataframe.\n",
    "    For example: If we have columns: Col_1, Col_2, Col_3 then it will calculate Col_1 - Col_2, Col_1 - Col_3, Col_2 - Col_3\n",
    "    This is to calculate the differnce in precipitation between different weeks of rainfall, like the difference in rainfall between week 17 and week 16.\n",
    "    \n",
    "    df: DataFrame\n",
    "    inter_cols: The columns for which the difference needs to be computed\n",
    "    \n",
    "    returns: New dataframe with the difference columns added to it.\n",
    "    '''\n",
    "    def apply_interacts(x, inter_cols):\n",
    "        cols = [x + '_diff_' + c for c in inter_cols[inter_cols.index(x)+1:]]\n",
    "        interacts_df[cols] = pd.concat([df[x] - df[c] for c in inter_cols[inter_cols.index(x)+1:]], axis=1)\n",
    "    \n",
    "    interacts_df = pd.DataFrame()\n",
    "    _ = df[inter_cols[:-1]].apply(lambda x: apply_interacts(x.name, inter_cols))\n",
    "    df = pd.concat([df, interacts_df], axis=1)\n",
    "\n",
    "    return df\n",
    "df = add_diff_interacts(df, precip_cols)\n",
    "\n",
    "step = 0.2\n",
    "to_bin = lambda x: np.floor(x / step) * step\n",
    "df[\"latbin\"] = df.X.map(to_bin)\n",
    "df[\"lonbin\"] = df.Y.map(to_bin)\n",
    "\n",
    "df['prev_x'] = df['X'] - 0.01\n",
    "df['next_x'] = df['X'] + 0.01\n",
    "df['prev_y'] = df['Y'] - 0.01\n",
    "df['next_y'] = df['Y'] + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:train.shape[0]].reset_index(drop=True)\n",
    "test = df[train.shape[0]:].reset_index(drop=True)\n",
    "features = [c for c in train.columns if c not in ['Square_ID','target_2015', 'geometry'] + precip_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': 186,\n",
    "     'min_data_in_leaf': 40, \n",
    "     'objective':'regression',\n",
    "     'max_depth': 18,\n",
    "     'learning_rate': 0.1,\n",
    "     \"boosting\": \"gbdt\",\n",
    "     \"feature_fraction\": 0.7,\n",
    "     \"metric\": 'rmse',\n",
    "     \"lambda_l1\": 1,\n",
    "     \"lambda_l2\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "fold n°0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.0894303\tvalid_1's rmse: 0.110288\n",
      "[200]\ttraining's rmse: 0.0821104\tvalid_1's rmse: 0.107409\n",
      "[300]\ttraining's rmse: 0.0793935\tvalid_1's rmse: 0.106368\n",
      "[400]\ttraining's rmse: 0.0793935\tvalid_1's rmse: 0.106368\n",
      "Early stopping, best iteration is:\n",
      "[283]\ttraining's rmse: 0.0793935\tvalid_1's rmse: 0.106368\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.0902419\tvalid_1's rmse: 0.108468\n",
      "[200]\ttraining's rmse: 0.0824377\tvalid_1's rmse: 0.105222\n",
      "[300]\ttraining's rmse: 0.0799003\tvalid_1's rmse: 0.10441\n",
      "[400]\ttraining's rmse: 0.0799003\tvalid_1's rmse: 0.10441\n",
      "Early stopping, best iteration is:\n",
      "[278]\ttraining's rmse: 0.0800464\tvalid_1's rmse: 0.104381\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.0916543\tvalid_1's rmse: 0.103157\n",
      "[200]\ttraining's rmse: 0.0838562\tvalid_1's rmse: 0.0999956\n",
      "[300]\ttraining's rmse: 0.0802629\tvalid_1's rmse: 0.0991362\n",
      "[400]\ttraining's rmse: 0.0800521\tvalid_1's rmse: 0.0990745\n",
      "[500]\ttraining's rmse: 0.0800521\tvalid_1's rmse: 0.0990745\n",
      "Early stopping, best iteration is:\n",
      "[325]\ttraining's rmse: 0.0800529\tvalid_1's rmse: 0.0990739\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.0894505\tvalid_1's rmse: 0.111567\n",
      "[200]\ttraining's rmse: 0.0817452\tvalid_1's rmse: 0.109018\n",
      "[300]\ttraining's rmse: 0.0801567\tvalid_1's rmse: 0.108463\n",
      "[400]\ttraining's rmse: 0.0801567\tvalid_1's rmse: 0.108463\n",
      "Early stopping, best iteration is:\n",
      "[253]\ttraining's rmse: 0.0801657\tvalid_1's rmse: 0.108455\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.0899957\tvalid_1's rmse: 0.108444\n",
      "[200]\ttraining's rmse: 0.0820961\tvalid_1's rmse: 0.105467\n",
      "[300]\ttraining's rmse: 0.0798765\tvalid_1's rmse: 0.104779\n",
      "[400]\ttraining's rmse: 0.0798765\tvalid_1's rmse: 0.104779\n",
      "Early stopping, best iteration is:\n",
      "[280]\ttraining's rmse: 0.0799929\tvalid_1's rmse: 0.104756\n",
      "CV score: 0.09803 \n"
     ]
    }
   ],
   "source": [
    "features = [c for c in train.columns if c not in ['Square_ID','target_2015', 'geometry'] + precip_cols]\n",
    "print(len(features))\n",
    "target = train['target_2015']\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "#### Our chosen hyperparameters were under-predicting the floods in some sense, the maximum flooding was around 0.8xx\n",
    "#### To remedy this underprediction we chose an arbitary number greater than one, i.e, 1.08, so our maximum flooding crossed 0.9\n",
    "#### Choosing a multiplier only changes the non-zero values.\n",
    "#### The '0' values or areas with no flood will not be affected as 0 x 1.08 = 0 still.\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx]*1.08)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx]*1.08)\n",
    "\n",
    "    num_round = 1500\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    predictions_lgb += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "oof_lgb[oof_lgb < 0] = 0\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb, target)**0.5))\n",
    "pt_1 = predictions_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV score: 0.09803 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "fold n°0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.0833703\tvalid_1's rmse: 0.102076\n",
      "[200]\ttraining's rmse: 0.0768773\tvalid_1's rmse: 0.0993429\n",
      "[300]\ttraining's rmse: 0.0751971\tvalid_1's rmse: 0.0987469\n",
      "[400]\ttraining's rmse: 0.0751971\tvalid_1's rmse: 0.0987469\n",
      "Early stopping, best iteration is:\n",
      "[273]\ttraining's rmse: 0.0752047\tvalid_1's rmse: 0.0987391\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.0837791\tvalid_1's rmse: 0.101076\n",
      "[200]\ttraining's rmse: 0.0772094\tvalid_1's rmse: 0.0981\n",
      "[300]\ttraining's rmse: 0.0752891\tvalid_1's rmse: 0.097519\n",
      "[400]\ttraining's rmse: 0.0752891\tvalid_1's rmse: 0.097519\n",
      "Early stopping, best iteration is:\n",
      "[279]\ttraining's rmse: 0.0752978\tvalid_1's rmse: 0.0975154\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.0852629\tvalid_1's rmse: 0.0958177\n",
      "[200]\ttraining's rmse: 0.0779838\tvalid_1's rmse: 0.0928295\n",
      "[300]\ttraining's rmse: 0.0756744\tvalid_1's rmse: 0.0921529\n",
      "[400]\ttraining's rmse: 0.0756744\tvalid_1's rmse: 0.0921529\n",
      "Early stopping, best iteration is:\n",
      "[277]\ttraining's rmse: 0.0757373\tvalid_1's rmse: 0.0921527\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.0841602\tvalid_1's rmse: 0.103503\n",
      "[200]\ttraining's rmse: 0.0768155\tvalid_1's rmse: 0.100807\n",
      "[300]\ttraining's rmse: 0.074819\tvalid_1's rmse: 0.100176\n",
      "[400]\ttraining's rmse: 0.074819\tvalid_1's rmse: 0.100176\n",
      "Early stopping, best iteration is:\n",
      "[257]\ttraining's rmse: 0.0748406\tvalid_1's rmse: 0.100164\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 200 rounds.\n"
     ]
    }
   ],
   "source": [
    "features = [c for c in train.columns if c not in ['Square_ID','target_2015', 'geometry'] + precip_cols]\n",
    "print(len(features))\n",
    "target = train['target_2015']\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx]*1)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx]*1)\n",
    "\n",
    "    num_round = 1500\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis = 0)\n",
    "\n",
    "    predictions_lgb += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "oof_lgb[oof_lgb < 0] = 0\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb*0.9, target)**0.5))\n",
    "\n",
    "\n",
    "#### Our chosen hyperparameters were under-predicting the floods in some sense, the maximum flooding was around 0.8xx\n",
    "#### To remedy this underprediction we chose an arbitary number greater than one, i.e, 1.08, so our maximum flooding crossed 0.9\n",
    "#### Choosing a multiplier only changes the non-zero values.\n",
    "#### The '0' values or areas with no flood will not be affected as 0 x 1.08 = 0 still.\n",
    "#### This is different than the previous case, because here we multiply the final predictions by 1.08, whereas previously we multiplied our target by 1.08\n",
    "\n",
    "pt_2 = predictions_lgb * 1.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV score: 0.09925 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Blending of different predictions\n",
    "predictions_lgb = pt_1 * 0.75 + pt_2 * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "fi = feature_importance_df.groupby('feature')['importance'].mean()\n",
    "fi.sort_values()[-20:].plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lgb[predictions_lgb < 0.2] = 0\n",
    "predictions_lgb[predictions_lgb > 1] = 1\n",
    "np.corrcoef(target, predictions_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectify_submission(best_sub):\n",
    "    \n",
    "    '''\n",
    "    This function is used to rectify the submissions based on EDA, and external data. \n",
    "    The documentation for the function can be found at: \" \"\n",
    "    wetlands: Gives the wetland distance of the place. The data source used is \" \", and is described at: \" \"\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    train['wetland_dist'] = wetlands['wetland_dist']\n",
    "    target = train['target_2015']\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        train, geometry=gpd.points_from_xy(train.X, train.Y))\n",
    "    gdf['target_2015'] = target\n",
    "    gdf['best_sub'] =  best_sub\n",
    "    best_sub_2 = best_sub.copy()\n",
    "    \n",
    "    #### Rectifications based on wetland distance. Places very near the wetlands are more likely to get flooded and vice versa.\n",
    "    #### The magnitues are in metres, So 5000 metres = 5 kilometres and 10000 metres = 10 kilometres\n",
    "    \n",
    "    sel = (best_sub_2 == 0)\n",
    "    best_sub_2[sel] = 0.03\n",
    "    sel = (train['wetland_dist'] > 5000) & (train['wetland_dist'] <= 10000)\n",
    "    best_sub_2[sel] = 0.015\n",
    "    sel = (train['wetland_dist'] > 10000)\n",
    "    best_sub_2[sel] = 0\n",
    "    sel = (train['wetland_dist'] == 0) & (train['target_2015'] == 1) & (train['elevation'] < 55)\n",
    "    best_sub_2[sel] = 0.94\n",
    "    \n",
    "    #### These regions are neither unflooded nor too much flooded, we blend them with the floods of 2015, by a very small amount to get a small boost\n",
    "    sel = (best_sub_2 > 0.1) & (best_sub_2 < 0.9)\n",
    "    best_sub_2[sel] = best_sub_2[sel] * 0.85 + train['target_2015'][sel] * 0.15\n",
    "    \n",
    "    ### The lower part of the map is mostly flooded, so we increase the flooding of the unflooded regions by a small percen (5 %).\n",
    "    ### This is again an arbitary number, and can further be tuned further\n",
    "    sel = (best_sub_2 > 0) & (best_sub_2 < 0.03) & (train.Y < 16.0)\n",
    "    print(sel.sum())\n",
    "    best_sub_2[sel] *= 1.05\n",
    "    \n",
    "    ### The eastern hemisphere is almost completely not flooded in our 2019 predictions, and we only see a negligibly small spot flooded.\n",
    "    ### To get rid of it, we set the flooding of the entire eastern region to 0.\n",
    "    gdf['rectified'] = best_sub_2\n",
    "    sel = (train.X >= 35.3) & (train.Y <= 15.9)\n",
    "    gdf.loc[sel, 'rectified'] = 0.00\n",
    "    return gdf['rectified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lgb = rectify_submission(predictions_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_FILE_NAME = OUTPUT_PATH\n",
    "sub=test[[\"Square_ID\"]]\n",
    "sub['target_2019'] = predictions_lgb\n",
    "sub.to_csv(SUB_FILE_NAME, index = False)\n",
    "sub['target_2019'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "create_download_link(filename = SUB_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "gdf['target_2019'] = predictions_lgb\n",
    "gdf['target_2015'] = target\n",
    "gdf['difference_in_predictions (2015 - 2019)'] = gdf['target_2015'] - gdf['target_2019']\n",
    "gdf.plot(column = 'target_2015', figsize=(6, 6), legend=True, ax = axes[0])\n",
    "gdf.plot(column = 'target_2019', figsize=(6, 6), legend=True, ax = axes[1])\n",
    "gdf.plot(column = 'difference_in_predictions (2015 - 2019)', legend=True, ax = axes[2])\n",
    "_ = axes[0].set_title('2019 Predictions')\n",
    "_ = axes[1].set_title('2015 Predictions')\n",
    "_ = axes[2].set_title('difference_in_predictions (2015 - 2019)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
