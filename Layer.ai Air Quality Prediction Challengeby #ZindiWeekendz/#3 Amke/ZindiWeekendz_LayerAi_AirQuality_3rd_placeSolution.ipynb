{"cells":[{"cell_type":"markdown","metadata":{"id":"433cc9be"},"source":["# Layer.ai Air Quality Prediction Challenge\n","# By Mohamed Eltayeb \u0026 Azer Ksouri"]},{"cell_type":"markdown","metadata":{"id":"BIJoWbtZTsY8"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1657873991829,"user":{"displayName":"Darius Moruri","userId":"07839644935093130146"},"user_tz":-180},"id":"0ejUG3OkTuAk","outputId":"971c6f20-e2ce-44ca-c3a1-8f57d5cb63d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Jul 15 08:33:11 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGxwn4TVTwON"},"outputs":[],"source":["%%capture\n","# Install Layer\n","!pip install -U layer -q\n","# Install Catboost\n","!pip install catboost --quiet"]},{"cell_type":"markdown","metadata":{"id":"4d141a81"},"source":["# Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91517d92"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import stats\n","\n","from sklearn.preprocessing import LabelEncoder\n","from catboost import CatBoostRegressor\n","from sklearn.compose import TransformedTargetRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","# Layer package\n","import layer\n","from layer.decorators import dataset,model, pip_requirements\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","pd.options.mode.chained_assignment = None  # default='warn'\n","pd.set_option('display.float_format', lambda x: '%.3f' % x)\n","pd.set_option('display.max_columns', None)"]},{"cell_type":"markdown","metadata":{"id":"oMPOGN8dRu0N"},"source":["# Login \u0026 register "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"8OrEb1UzRxCS"},"outputs":[{"name":"stdout","output_type":"stream","text":["Please open the following link in your web browser. Once logged in, copy the code and paste it here.\n","https://app.layer.ai/oauth/authorize?response_type=code\u0026code_challenge=nNGt1O8Nr6qcEmaFLq_8409194HI8IbfEHwOYFNUjbQ\u0026code_challenge_method=S256\u0026client_id=0STDdcnpK48P8A429EAAn93WNuLmViLR\u0026redirect_uri=https://app.layer.ai/oauth/code\u0026scope=offline_access\u0026audience=https://app.layer.ai\n"]}],"source":["layer.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZRS5Ju0SEL7"},"outputs":[],"source":["layer.init(\"Air_Quality_Prediction_Challenge_Exp_Base\")"]},{"cell_type":"markdown","metadata":{"id":"0559ab45"},"source":["## Define Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3e964d67"},"outputs":[],"source":["#Group Time Series Split\n","from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n","from sklearn.utils.validation import _deprecate_positional_args\n","\n","class GroupTimeSeriesSplit(_BaseKFold):\n","    @_deprecate_positional_args\n","    def __init__(self,\n","                 n_splits=5,\n","                 *,\n","                 max_train_size=None\n","                 ):\n","        super().__init__(n_splits, shuffle=False, random_state=None)\n","        self.max_train_size = max_train_size\n","\n","    def split(self, X, y=None, groups=None):\n","        \"\"\"Generate indices to split data into training and test set.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Training data, where n_samples is the number of samples\n","            and n_features is the number of features.\n","        y : array-like of shape (n_samples,)\n","            Always ignored, exists for compatibility.\n","        groups : array-like of shape (n_samples,)\n","            Group labels for the samples used while splitting the dataset into\n","            train/test set.\n","        Yields\n","        ------\n","        train : ndarray\n","            The training set indices for that split.\n","        test : ndarray\n","            The testing set indices for that split.\n","        \"\"\"\n","        if groups is None:\n","            raise ValueError(\n","                \"The 'groups' parameter should not be None\")\n","        X, y, groups = indexable(X, y, groups)\n","        n_samples = _num_samples(X)\n","        n_splits = self.n_splits\n","        n_folds = n_splits + 1\n","        group_dict = {}\n","        u, ind = np.unique(groups, return_index=True)\n","        unique_groups = u[np.argsort(ind)]\n","        n_samples = _num_samples(X)\n","        n_groups = _num_samples(unique_groups)\n","        for idx in np.arange(n_samples):\n","            if (groups[idx] in group_dict):\n","                group_dict[groups[idx]].append(idx)\n","            else:\n","                group_dict[groups[idx]] = [idx]\n","        if n_folds \u003e n_groups:\n","            raise ValueError(\n","                (\"Cannot have number of folds={0} greater than\"\n","                 \" the number of groups={1}\").format(n_folds,\n","                                                     n_groups))\n","        group_test_size = n_groups // n_folds\n","        group_test_starts = range(n_groups - n_splits * group_test_size,\n","                                  n_groups, group_test_size)\n","        for group_test_start in group_test_starts:\n","            train_array = []\n","            test_array = []\n","            for train_group_idx in unique_groups[:group_test_start]:\n","                train_array_tmp = group_dict[train_group_idx]\n","                train_array = np.sort(np.unique(\n","                                      np.concatenate((train_array,\n","                                                      train_array_tmp)),\n","                                      axis=None), axis=None)\n","            train_end = train_array.size\n","            if self.max_train_size and self.max_train_size \u003c train_end:\n","                train_array = train_array[train_end -\n","                                          self.max_train_size:train_end]\n","            for test_group_idx in unique_groups[group_test_start:\n","                                                group_test_start +\n","                                                group_test_size]:\n","                test_array_tmp = group_dict[test_group_idx]\n","                test_array = np.sort(np.unique(\n","                                              np.concatenate((test_array,\n","                                                              test_array_tmp)),\n","                                     axis=None), axis=None)\n","            yield [int(i) for i in train_array], [int(i) for i in test_array]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"65672b87"},"outputs":[],"source":["#Plot the LGBM Features Importances\n","def plotImp(model, X , num = 20, fig_size = (40, 20)):\n","    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n","    plt.figure(figsize=fig_size)\n","    sns.set(font_scale = 5)\n","    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n","                                                        ascending=False)[0:num])\n","    plt.title('LightGBM Features (avg over folds)')\n","    plt.tight_layout()\n","    plt.savefig('lgbm_importances-01.png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"449cb28d"},"outputs":[],"source":["#Label Encoder\n","def label_enc(train_df, test_df, features):\n","    lbl_enc = LabelEncoder()\n","    full_data = pd.concat([train_df[features], test_df[features]],axis=0)\n","    for col in (features):\n","        print(col)\n","        lbl_enc.fit(full_data[col].values)\n","        train_df[col] = lbl_enc.transform(train_df[col])\n","        test_df[col] = lbl_enc.transform(test_df[col])\n","    return train_df, test_df"]},{"cell_type":"markdown","metadata":{"id":"a49c11da"},"source":["# Read the training and testing data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"addd163a"},"outputs":[],"source":["train_df  = layer.get_dataset(\"zindi/air-quality/datasets/train\").to_pandas()\n","test_df  = layer.get_dataset(\"zindi/air-quality/datasets/test\").to_pandas()\n","sample_submission = layer.get_dataset(\"zindi/air-quality/datasets/sample_submission\").to_pandas()"]},{"cell_type":"markdown","metadata":{"id":"be527b52"},"source":["# Add The Time Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4386282b"},"outputs":[],"source":["train_df = train_df.sort_values(['date','device']).reset_index(drop=True) \n","test_df = test_df.sort_values(['date','device']).reset_index(drop=True)\n","\n","for dataset in (train_df,test_df):\n","    dataset['date'] = pd.to_datetime(dataset['date'])\n","    dataset['Day'] = dataset.date.dt.day\n","    dataset['Month'] = dataset.date.dt.month\n","    dataset['Year'] = dataset.date.dt.year\n","    dataset['DayOfWeek'] = dataset.date.dt.dayofweek\n","    dataset['DayOfYear'] = dataset.date.dt.dayofyear\n","    dataset['Week'] = dataset.date.dt.weekofyear\n","    dataset.set_index('date', inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5f92c3a"},"outputs":[],"source":["ID = test_df['ID']\n","test_df.drop('ID',inplace=True,axis=1)\n","train_df.drop('ID',inplace=True,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"53857f36"},"source":["# Exploratory data analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbfef88f"},"outputs":[],"source":["train_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ab6a206"},"outputs":[],"source":["test_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ead70d8"},"outputs":[],"source":["train_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39ee4bed"},"outputs":[],"source":["test_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"97a57f21"},"outputs":[],"source":["train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9d06d557"},"outputs":[],"source":["test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"635d4e82"},"outputs":[],"source":["train_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"babc9336"},"outputs":[],"source":["test_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfb6d278"},"outputs":[],"source":["#The cardinality of each catgorical feature (Training)\n","cat_cols = train_df.columns\n","for col in cat_cols:\n","    print(col, train_df[col].nunique())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2070c323"},"outputs":[],"source":["#The cardinality of each catgorical feature (Testing)\n","cat_cols = test_df.columns\n","for col in cat_cols:\n","    print(col, test_df[col].nunique())"]},{"cell_type":"markdown","metadata":{"id":"e0f7b047"},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"a5d49c54"},"source":["# Missing Data "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20c8f29a"},"outputs":[],"source":["#missing data percentage (Training)\n","total = train_df.isnull().sum().sort_values(ascending=False)\n","percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n","percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n","missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n","missing_data.head(60)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35c2dd0c"},"outputs":[],"source":["#missing data percentage (Testing)\n","total = test_df.isnull().sum().sort_values(ascending=False)\n","percent_1 = test_df.isnull().sum()/test_df.isnull().count()*100\n","percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n","missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n","missing_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9783ebdf"},"outputs":[],"source":["train_df = train_df.bfill().ffill()\n","test_df = test_df.bfill().ffill()"]},{"cell_type":"markdown","metadata":{"id":"9f685bf6"},"source":["# Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"28f4c4a2"},"source":["## - Lags Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"37916f9d"},"outputs":[],"source":["def LAG(data,LagFeature,shift=1,NewFeatures=[]) :\n","    data[NewFeatures[0]]   = data[LagFeature]  - data[LagFeature].shift(shift)\n","    data[NewFeatures[1]]   = data[LagFeature].shift(shift)\n","\n","num_feats = train_df.columns\n","num_feats = num_feats.drop(['Week','DayOfYear','DayOfWeek','Year','Month','Day','pm2_5','temp_mean','humidity','site_longitude','site_latitude','device'])\n","\n","for feature in num_feats:\n","    LAG(train_df,LagFeature=f'{feature}',shift=1,NewFeatures=[f'{feature}_diff_Lag1',f'{feature}_Lag1'])\n","    LAG(test_df,LagFeature=f'{feature}',shift=1,NewFeatures=[f'{feature}_diff_Lag1',f'{feature}_Lag1'])"]},{"cell_type":"markdown","metadata":{"id":"4b64caeb"},"source":["## - Combination Between Time Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cba07fe"},"outputs":[],"source":["for dataset in (train_df,test_df):\n","    dataset['Year_Month'] = dataset['Year'].astype(str) + '-' + dataset['Month'].astype(str)\n","    dataset['Year_Week'] = dataset['Year'].astype(str) + '-' + dataset['Week'].astype(str)\n","    dataset['Year_Month_Day'] = dataset['Year'].astype(str) + '-' + dataset['Month'].astype(str) + '-' + dataset['Day'].astype(str)\n","    \n","feats = ['Year_Month','Year_Week','Year_Month_Day']\n","train_df,test_df = label_enc(train_df,test_df,feats)"]},{"cell_type":"markdown","metadata":{"id":"6099d339"},"source":["## - Aggregations Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ba255c96"},"outputs":[],"source":["DevicePM2_5Mean = dict(train_df.groupby('device')['pm2_5'].mean())\n","DevicePM2_5Std = dict(train_df.groupby('device')['pm2_5'].std())\n","DevicePM2_5Min = dict(train_df.groupby('device')['pm2_5'].min())\n","DevicePM2_5Max = dict(train_df.groupby('device')['pm2_5'].max())\n","\n","for dataset in (train_df,test_df):\n","    dataset['DevicePM2_5Mean'] = dataset['device'].map(DevicePM2_5Mean)\n","    dataset['DevicePM2_5Std'] = dataset['device'].map(DevicePM2_5Std)\n","    dataset['DevicePM2_5Min'] = dataset['device'].map(DevicePM2_5Min)\n","    dataset['DevicePM2_5Max'] = dataset['device'].map(DevicePM2_5Max)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2683617"},"outputs":[],"source":["def Agg(Feature):\n","    for dataset in (train_df,test_df):\n","        dataset[f'{Feature}PerMonth'] = dataset['Month'].map(dict(dataset.groupby('Month')[Feature].mean()))\n","        dataset[f'{Feature}PerWeek'] = dataset['Year_Week'].map(dict(dataset.groupby('Year_Week')[Feature].mean()))\n","        dataset[f'{Feature}PerDay'] = dataset['Year_Month_Day'].map(dict(dataset.groupby('Year_Month_Day')[Feature].mean()))\n","        \n","        dataset[f'{Feature}Month_std'] = dataset['Month'].map(dict(dataset.groupby('Month')[Feature].std()))\n","        dataset[f'{Feature}Week_std'] = dataset['Year_Week'].map(dict(dataset.groupby('Year_Week')[Feature].std()))\n","        dataset[f'{Feature}Day_std'] = dataset['Year_Month_Day'].map(dict(dataset.groupby('Year_Month_Day')[Feature].std()))\n","        \n","        dataset[f'{Feature}Month_min'] = dataset['Month'].map(dict(dataset.groupby('Month')[Feature].min()))\n","        dataset[f'{Feature}Week_min'] = dataset['Year_Week'].map(dict(dataset.groupby('Year_Week')[Feature].min()))\n","        dataset[f'{Feature}Day_min'] = dataset['Year_Month_Day'].map(dict(dataset.groupby('Year_Month_Day')[Feature].min()))\n","       \n","        dataset[f'{Feature}Month_max'] = dataset['Month'].map(dict(dataset.groupby('Month')[Feature].max()))\n","        dataset[f'{Feature}Week_max'] = dataset['Year_Week'].map(dict(dataset.groupby('Year_Week')[Feature].max()))\n","        dataset[f'{Feature}Day_max'] = dataset['Year_Month_Day'].map(dict(dataset.groupby('Year_Month_Day')[Feature].max()))\n","        \n","Agg('temp_mean')\n","Agg('humidity')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bd36e4e"},"outputs":[],"source":["train_df.drop(['Year_Month','Year_Week','Year_Month_Day'],inplace=True,axis=1)\n","test_df.drop(['Year_Month','Year_Week','Year_Month_Day'],inplace=True,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"ff0a943e"},"source":["## - Rolling Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17e1a9ec"},"outputs":[],"source":["def Rolling(feature):\n","    for dataset in (train_df,test_df):\n","        dataset[f'{feature}_Rolling_3'] = dataset[feature].rolling(3).mean()\n","        dataset[f'{feature}_Rolling_5'] = dataset[feature].rolling(5).mean()\n","\n","        dataset[f\"{feature}_rolling_mean_60\"] = dataset.rolling(60).mean()[feature]\n","        dataset[f\"{feature}_rolling_max_60\"] = dataset.rolling(60).max()[feature]\n","        dataset[f\"{feature}_rolling_min_60\"] = dataset.rolling(60).min()[feature]\n","\n","        dataset[f\"{feature}_rolling_mean_30\"] = dataset.rolling(30).mean()[feature]\n","        dataset[f\"{feature}_rolling_max_30\"] = dataset.rolling(30).max()[feature]\n","        dataset[f\"{feature}_rolling_min_30\"] = dataset.rolling(30).min()[feature]\n","\n","        dataset[f\"{feature}_rolling_mean_10\"] = dataset.rolling(10).mean()[feature]\n","        dataset[f\"{feature}_rolling_max_10\"] = dataset.rolling(10).max()[feature]\n","        dataset[f\"{feature}_rolling_min_10\"] = dataset.rolling(10).min()[feature]\n","\n","Rolling('temp_mean')\n","Rolling('humidity')"]},{"cell_type":"markdown","metadata":{"id":"a6f2c5fd"},"source":["## - Polar Coordinates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8b6d7d72"},"outputs":[],"source":["def Polar(X,y, a = 0, b = 0): # a and b represnt the center\n","    r = np.sqrt((X-a)**2 + (y-b)**2)\n","    phi = np.arctan2((y-a), (X-b))\n","    return r, phi\n","\n","train_df['R'], train_df['Phi'] = Polar(train_df[\"site_latitude\"],train_df[\"site_longitude\"])\n","test_df['R'], test_df['Phi'] = Polar(test_df[\"site_latitude\"],test_df[\"site_longitude\"])"]},{"cell_type":"markdown","metadata":{"id":"e94bd15d"},"source":["## - Foureier Frequnecies and Amplitudes For Features That Contain Seasonality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3fc6e8b"},"outputs":[],"source":["freq2_dict_no_log = dict()\n","freq3_dict_no_log = dict()\n","\n","amp2_dict_no_log = dict()\n","amp3_dict_no_log = dict()\n","\n","for feat_1 in ('Year','Month','Day'):\n","    for feat_2 in ('temp_mean', 'humidity'):\n","        for i in range(min(train_df[feat_1].unique()), max(train_df[feat_1].unique()) + 1):\n","\n","            a = train_df.loc[train_df[feat_1]==i]\n","            a_sales = a[feat_2]\n","\n","            Y = np.fft.fft(a_sales.values)\n","            Y = abs(Y)\n","            freq = np.fft.fftfreq(len(Y), 1)\n","\n","            intercept_index = np.argmax(Y)\n","            Y = np.delete(Y, intercept_index)\n","            freq = np.delete(freq, intercept_index)\n","\n","            amplitude_1_index = np.argmax(Y)\n","            amplitude_1 = Y[amplitude_1_index]\n","            Y = np.delete(Y, amplitude_1_index)\n","            freq_1 = freq[amplitude_1_index]\n","            freq = np.delete(freq, amplitude_1_index)\n","\n","            amplitude_2_index = np.argmax(Y)\n","            amplitude_2 = Y[amplitude_2_index]\n","            Y = np.delete(Y, amplitude_2_index)\n","            freq_2 = freq[amplitude_2_index]\n","            freq = np.delete(freq, amplitude_2_index)\n","\n","            amplitude_3_index = np.argmax(Y)\n","            amplitude_3 = Y[amplitude_3_index]\n","            Y = np.delete(Y, amplitude_3_index)\n","            freq_3 = freq[amplitude_3_index]\n","            freq = np.delete(freq, amplitude_3_index)\n","\n","            #Freq_1 is not included because it seems as it is always 0\n","            a[f'Frequency_2_{feat_1}_{feat_2}'] = freq_2\n","            a[f'Frequency_3_{feat_1}_{feat_2}'] = freq_3\n","\n","            a[f'Amplitude_2_{feat_1}_{feat_2}'] = amplitude_2\n","            a[f'Amplitude_3_{feat_1}_{feat_2}'] = amplitude_3\n","\n","\n","            freq2_dict_no_log[i] = freq_2\n","            freq3_dict_no_log[i] = freq_3\n","\n","            amp2_dict_no_log[i] = amplitude_2\n","            amp3_dict_no_log[i] = amplitude_3\n","\n","\n","            if i == min(train_df[feat_1].unique()):\n","                k = a\n","            else:\n","                k = pd.concat([k,a])\n","                \n","        train_df = k\n","        \n","        test_df[f'Frequency_2_{feat_1}_{feat_2}'] = test_df[feat_1].map(freq2_dict_no_log)\n","        test_df[f'Frequency_3_{feat_1}_{feat_2}'] = test_df[feat_1].map(freq3_dict_no_log)\n","        test_df[f'Amplitude_2_{feat_1}_{feat_2}'] = test_df[feat_1].map(amp2_dict_no_log)\n","        test_df[f'Amplitude_3_{feat_1}_{feat_2}'] = test_df[feat_1].map(amp3_dict_no_log)\n","        \n","        freq2_dict_no_log = dict()\n","        freq3_dict_no_log = dict()\n","        amp2_dict_no_log = dict()\n","        amp3_dict_no_log = dict()"]},{"cell_type":"markdown","metadata":{"id":"87318634"},"source":["## - Percentage change in Temperature and Humidity "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aa61001b"},"outputs":[],"source":["periods = [1, 3, 7, 14]\n","for period in periods:\n","    train_df.loc[:, f\"PctChangeTemp_{period}\"] = train_df[\"temp_mean\"].pct_change(period)\n","    train_df.loc[:, f\"PctChangeHumi_{period}\"] = train_df[\"humidity\"].pct_change(period)\n","    test_df.loc[:, f\"PctChangeTemp_{period}\"] = test_df[\"temp_mean\"].pct_change(period)\n","    test_df.loc[:, f\"PctChangeHumi_{period}\"] = test_df[\"humidity\"].pct_change(period)"]},{"cell_type":"markdown","metadata":{"id":"4559cc2c"},"source":["## - Historic Volatility"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"389e95e9"},"outputs":[],"source":["periods = [3, 7, 14]\n","for period in periods:\n","    train_df.loc[:, f\"volatility_temp_mean_{period}\"] = train_df[\"temp_mean\"].diff().rolling(period).std()\n","    test_df.loc[:, f\"volatility_temp_mean_{period}\"] = test_df[\"temp_mean\"].diff().rolling(period).std()\n","    train_df.loc[:, f\"volatility_humidity_{period}\"] = train_df[\"humidity\"].diff().rolling(period).std()\n","    test_df.loc[:, f\"volatility_humidity_{period}\"] = test_df[\"humidity\"].diff().rolling(period).std()"]},{"cell_type":"markdown","metadata":{"id":"5c797fd7"},"source":["# Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d78094ec"},"outputs":[],"source":["train_df, test_df = label_enc(train_df,test_df,['device'])"]},{"cell_type":"markdown","metadata":{"id":"e52efdb8"},"source":["# Modeling"]},{"cell_type":"markdown","metadata":{"id":"modOc2hdUzZu"},"source":["## Validate our model ; Layer.ai "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvHU8RrgU6rv"},"outputs":[],"source":["@model(\"air_model\")\n","@pip_requirements(packages=[\"catboost\",\"seaborn\"])\n","def train():\n","    from catboost import CatBoostRegressor\n","    from sklearn import metrics\n","    from sklearn.compose import TransformedTargetRegressor\n","    import numpy as np\n","    import pandas as pd\n","    import matplotlib.pyplot as plt\n","    import seaborn as sns\n","\n","    Means = [20.772597188397544]\n","    STDs = [2.2423317130434035]\n","\n","    params = {'n_estimators': 2064, 'learning_rate': 0.03196897706232692, 'depth': 5, 'reg_lambda': 12.680808984686983,}\n","    cb = CatBoostRegressor(**params,verbose=0, random_state=42)\n","    LOGcb = TransformedTargetRegressor(cb, func = np.log1p, inverse_func = np.expm1)\n","\n","    X = train_df.drop('pm2_5',axis=1).values\n","    y = train_df['pm2_5'].values\n","\n","    temp = train_df.copy()\n","    temp['date'] = temp.index\n","    temp = temp.reset_index(drop=True)\n","    \n","    layer.log({\"model_params\":params})\n","\n","    scores = []\n","    for fold_ , (train_index, test_index) in enumerate(GroupTimeSeriesSplit(n_splits=4).split(X, y, groups=temp['date'].values)):\n","        X_Train, X_Test = X[train_index], X[test_index]\n","        y_Train, y_Test = y[train_index], y[test_index]\n","        LOGcb.fit(X_Train,y_Train)\n","        vali = temp.loc[(temp.index \u003e= test_index[0]) \u0026 (temp.index \u003c= test_index[-1])]\n","        y_pred = LOGcb.predict(X_Test)\n","        scores.append(mean_absolute_error(y_pred,y_Test))\n","\n","        layer.log({f'Validation Step {fold_} \u003c==\u003e Mean Absolute Error':metrics.mean_absolute_error(y_Test, y_pred)})\n","        layer.log({f'Validation Step {fold_} \u003c==\u003e Mean Squarred Error': metrics.mean_squared_error(y_Test, y_pred)})\n","        layer.log({f'Validation Step {fold_} \u003c==\u003e Root Mean Squared Error': np.sqrt(metrics.mean_squared_error(y_Test, y_pred))})\n","        print(scores[-1])\n","\n","        print(\"\\nMean:\",np.mean(scores),\"\\nSTD: \", np.std(scores))\n","        Means.append(np.mean(scores))\n","        STDs.append(np.std(scores))\n","\n","        if (Means[-1] \u003c Means[-2]):\n","            print('Better')\n","        elif (Means[-1] \u003e Means[-2]):\n","            print('Worse')\n","        else:\n","            print('Same')\n","\n","        if   (STDs[-1] \u003e STDs[-2]):\n","            print('Worse')\n","        elif (STDs[-1] \u003c STDs[-2]):\n","            print('Better')\n","        else:\n","            print('Same')\n","\n","    #Plot the Catboost Features Importances\n","    cb.fit(X,y)\n","    feature_imp = pd.DataFrame({'Value':cb.feature_importances_,'Feature':train_df.drop('pm2_5',axis=1).columns})\n","    plt.figure(figsize=(40, 20))\n","    sns.set(font_scale = 5)\n","    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n","                                                        ascending=False)[0:20])\n","    plt.title('Feature Imprtances (avg over folds)')\n","    plt.tight_layout()\n","    layer.log({\"Feature importance\": plt.gcf()})\n","\n","    return cb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1L0-AyZU_cX"},"outputs":[],"source":["layer.run([train])"]},{"cell_type":"markdown","metadata":{"id":"rYSzOkL_U7wH"},"source":["## Train On Different Seeds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fc13d03f"},"outputs":[],"source":["#Averaging the predictions of the same model with different seeds to get more consistent results\n","X = train_df.drop('pm2_5',axis=1)\n","y = train_df['pm2_5']\n","\n","Predictions = pd.DataFrame()\n","\n","for seed in range(20,46):\n","    print(f'Seed: {seed}')\n","    params = {'n_estimators': 2064, 'learning_rate': 0.03196897706232692, 'depth': 5, 'reg_lambda': 12.680808984686983}\n","    CB = CatBoostRegressor(**params,verbose=0, random_state=seed, task_type = 'GPU')\n","    LogCB = TransformedTargetRegressor(CB, func = np.log1p, inverse_func = np.expm1)  \n","    LogCB.fit(X, y)\n","\n","    Predictions[f'Target_{seed}'] = LogCB.predict(test_df)\n","    Predictions[f'Target_{seed}'] = Predictions[f'Target_{seed}'] * 0.975  #A Correction Factor of 0.975\n","    \n","#Averaging the Results\n","Predictions['Mean'] = Predictions.mean(axis=1)\n","Predictions['HMean'] = Predictions.apply(stats.hmean, axis=1)\n","Predictions['GMean'] = Predictions.apply(stats.gmean, axis=1)\n","\n","#Averaging the Second Results\n","FinalPred = Predictions[['Mean','HMean','GMean']].apply(stats.hmean,axis=1)\n","\n","#Making the submission file\n","submission = pd.DataFrame({\"Id\": ID ,\"pm2_5\": FinalPred.values})\n","submission.to_csv('AirQualityPrediction.csv',index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"name":"ZindiWeekendz_LayerAi_AirQuality_3rd_placeSolution.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}