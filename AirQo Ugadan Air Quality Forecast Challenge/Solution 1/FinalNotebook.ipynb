{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTXUdmIswY74"
   },
   "source": [
    "## Run these cells only once during a session\n",
    "\n",
    "Before running these files, there should be 3 importants thing already living within the runtime / directory space\n",
    "\n",
    "1. `mlod.zip` - this zip file contains the entire code base\n",
    "2. `requirements.txt` - the list of packages to install before doing anything\n",
    "3. this notebook file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp2M5cLFwY78"
   },
   "source": [
    "### Installing the `mlod` package\n",
    "\n",
    "This cell installs the mlod package to the Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58GVmoI3wY79"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('./mlod.zip', 'r') as zpf:\n",
    "    zpf.extractall(path='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vclA-R0BwY79"
   },
   "source": [
    "### Installing the packages\n",
    "Run the cell below to install the packages used in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTTUc-LuwY7-"
   },
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1HNmpIpwY7-"
   },
   "source": [
    "# Challenge #1: AirQo Ugandan Air Quality Forecast Challenge\n",
    "\n",
    "This notebook contains the reformat of the code, so that properly set up for implementation.\n",
    "Most of the code abstractions are written inside our package `mlod`, which should be included with this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdmgxzmQwY7_"
   },
   "source": [
    "## Init Steps\n",
    "\n",
    "This section involves setting up the data from the `zindi` to use for the competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3ZGB11QwY7_"
   },
   "source": [
    "### Setting up the data\n",
    "\n",
    "Please upload the to the `./data` path inside the workspace folder. Run the cell below, repeateadly till when there are no errors.\n",
    "Make sure the uploaded data is the `Train.csv` and `Test.csv` used in the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYKyxQwHwY7_"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "train_file_csv = Path('./data/Train.csv')\n",
    "test_file_csv = Path('./data/Test.csv')\n",
    "\n",
    "# check if Train file doesn't exist\n",
    "assert train_file_csv.exists(), 'Make sure the Test csv file exists the path \"%s\"' % train_file_csv\n",
    "\n",
    "# check if Test file doesn't exist\n",
    "assert test_file_csv.exists(), 'Make sure the Test csv file exists the path \"%s\"' % test_file_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J86xRlRQwY8A"
   },
   "source": [
    "## Actual sequence of processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWHWia7HwY8A"
   },
   "source": [
    "### Initiating different processes\n",
    "\n",
    "Performing steps that are needed for before doing any form of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jC7IUwCwY8B"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# using our chosen seed number\n",
    "from mlod import SEED_NUMBER as MLOD_SEED_NUMBER\n",
    "\n",
    "# Setting the seed\n",
    "random.seed(MLOD_SEED_NUMBER)\n",
    "np.random.seed(MLOD_SEED_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0Ebm8fswY8B"
   },
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4NjDfN5wY8B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Fetching the data\n",
    "train_df = pd.read_csv(train_file_csv)\n",
    "test_df = pd.read_csv(test_file_csv)\n",
    "\n",
    "TEST_IDS = test_df['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZRT-U8GwY8C"
   },
   "source": [
    "\n",
    "### Preprocessing the data\n",
    "\n",
    "[Low-level Preprocessing]<br />\n",
    "By using the `mlod.preprocessors.*` involves preprocessing the data in the following ways\n",
    "- Modifying the data such that each row, has its atomic values, thus making the data **grow** in size\n",
    "- Performing **special** feature engineering that some of which include:\n",
    "    - Peforming Cyclic Representation to selected features\n",
    "    - Using wind speed (`wind_spd`) and direction (`wind_dir`) to obtain \n",
    "        catersian values for speed (`u` and `v`)\n",
    "    - Add past features within a certain window of an the current row.<br />\n",
    "        This technique is to help data make model make relation between sequence of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk5eHl_4wY8C"
   },
   "source": [
    "### Preprocess + Model Training\n",
    "\n",
    "Since out approach is an ensemble, and the different models are preprocessed differently, the code below, contains the `Model` paired with its `PreProcessor`.\n",
    "\n",
    "Since for our ensemble we are boosting, we will be using the values from the `1`st process and feed it to the next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx7XbTG4wY8C"
   },
   "source": [
    "#### 1: LightGBM + Version 1 Pre Processing\n",
    "\n",
    "This first approach includes using our `MlodPreProcessor` and our `LGBModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfaoy7I3wY8D"
   },
   "outputs": [],
   "source": [
    "from mlod.preprocessors import MlodPreProcessor\n",
    "\n",
    "mlod_preprocessor = MlodPreProcessor()\n",
    "mlod_pp_opts = dict(cols_to_retain=['ID'])\n",
    "x_train, y_train = mlod_preprocessor.process(train_df, **mlod_pp_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lehxK5b0wY8D"
   },
   "outputs": [],
   "source": [
    "# Training the LGB Model\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "from mlod.models import LGBModel\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "lgb_model = LGBModel('airqo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bZGPkGOwY8D"
   },
   "outputs": [],
   "source": [
    "x_train_ids = x_train.pop('ID')\n",
    "fold_group = x_train['day_idx'].astype(str) + '_' + x_train['24hr_idx'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvWs8FMiwY8E"
   },
   "outputs": [],
   "source": [
    "assert 'ID' not in x_train.columns, 'Make sure ID is NOT in the columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISKLkWa-wY8E"
   },
   "outputs": [],
   "source": [
    "# perfoming evalution, not training since 'cv' is True\n",
    "lgb_eval_out = lgb_model.train(x_train, y_train, cv=True, kfold=GroupKFold, group=fold_group, n_splits=3)\n",
    "\n",
    "# Save the to feed to the next\n",
    "df_to_feed = pd.DataFrame.from_dict({ 'ID': x_train_ids, 'oof': lgb_eval_out['oof'] })\n",
    "\n",
    "save_path = './lgb_eval.csv'\n",
    "df_to_feed.to_csv(save_path)\n",
    "print('Saving the OOF values to path: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rB7uOPrfwY8E"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# training the model\n",
    "lgb_model.train(x_train, y_train, cv=False)\n",
    "\n",
    "# save the model\n",
    "lgb_model.model.save_model('./lgb-airqo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZzicXyXwY8F"
   },
   "source": [
    "#### 2: CatBoost + Version 2 Pre Processing\n",
    "\n",
    "This first approach includes using our `AirQoPreProcessor` and our `CatBoostModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgsAK0n2wY8F"
   },
   "outputs": [],
   "source": [
    "## Training the CatBoost Model\n",
    "# ------------------------------\n",
    "from mlod.preprocessors import AirQoPreProcessor\n",
    "\n",
    "airqo_preprocessor = AirQoPreProcessor()\n",
    "\n",
    "airqo_pp_opts = dict(cols_to_retain=['ID'])\n",
    "x_train, y_train = airqo_preprocessor.process(train_df, **airqo_pp_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UOQjX75wY8G"
   },
   "outputs": [],
   "source": [
    "# since the output of the LGBModel prediction have way more rows (121x) because \n",
    "#  of the way it was preprocessed, we need to deal with this\n",
    "train_feed = df_to_feed.groupby('ID').mean()\n",
    "\n",
    "# Add the feed value to the data before training to new model\n",
    "x_train = x_train.join(train_feed, on='ID')\n",
    "\n",
    "# drop the ID after joining\n",
    "del x_train['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhelANgYwY8G"
   },
   "outputs": [],
   "source": [
    "from mlod.models import CatBoostModel\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cb_model = CatBoostModel('airqo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z53IlNRxwY8G"
   },
   "outputs": [],
   "source": [
    "# performing cross validation training\n",
    "cb_eval_out = cb_model.train(x_train, y_train, cv=True, store_cv_models=True, kfold=KFold, n_splits=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4R50zVkwY8H"
   },
   "source": [
    "### Ensemble Prediction\n",
    "\n",
    "Since we are dealing with an ensemble model, the prediction will most likely also have to be different.\n",
    "We would need to take the output of `lgb_model` and use it as an input to the `cb_model`.\n",
    "\n",
    "Below is a function that captures this ensemble prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wdZBLN1wY8H"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mlod.models import Model\n",
    "from mlod.preprocessors import PreProcessor\n",
    "from typing import Tuple\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('mlod')\n",
    "\n",
    "class EnsemblePredictor:\n",
    "    def __init__(self, \n",
    "                 trained_lgb_model: Model, \n",
    "                 cv_trained_cb_model: Model, \n",
    "                 lgb_pp_opts: Tuple[PreProcessor, dict], \n",
    "                 cb_pp_opts: Tuple[PreProcessor, dict]):\n",
    "        \n",
    "        # Checks if the models are trained\n",
    "        assert trained_lgb_model.model is not None, \"the lgb model is not trained\"\n",
    "        assert cv_trained_cb_model.is_cv_trained, \"the cb model needs to be trained by cross validation\"\n",
    "        \n",
    "        self.lgb = trained_lgb_model\n",
    "        self.cb = cv_trained_cb_model\n",
    "        \n",
    "        # load up the preprocessor and config used in LGB model\n",
    "        lgb_pp, lgp_opts = lgb_pp_opts\n",
    "        self.lgb_pp = lgb_pp\n",
    "        self.lgp_opts = lgp_opts\n",
    "        \n",
    "        # load up the preprocessor and config used in CatBoost model\n",
    "        cb_pp, cb_opts = cb_pp_opts\n",
    "        self.cb_pp = cb_pp\n",
    "        self.cb_opts = cb_opts\n",
    "        \n",
    "    def predict(self, x: pd.DataFrame) -> np.ndarray:\n",
    "        \n",
    "        # pre-process like lgb\n",
    "        x_out_lgb = self.lgb_pp.process(x.copy(), test=True, **self.lgp_opts)\n",
    "        x_ids = x_out_lgb.pop('ID')\n",
    "        \n",
    "        # pre-process like cb\n",
    "        x_out_cb = self.cb_pp.process(x.copy(), test=True, **self.cb_opts)\n",
    "        \n",
    "        logger.info('Making prediction using base model')\n",
    "        # output for the lgb + merge with x_out_cb\n",
    "        to_merge = pd.DataFrame.from_dict({ 'ID': x_ids, 'oof': self.lgb.predict(x_out_lgb) })\n",
    "        \n",
    "        # mean merge the values\n",
    "        to_merge = to_merge.groupby('ID').mean()\n",
    "        x_out_cb = x_out_cb.join(to_merge, on='ID')\n",
    "        \n",
    "        # remove ID col + empty unneeded data\n",
    "        del x_out_cb['ID']\n",
    "        del to_merge\n",
    "        \n",
    "        # store the list of predictions\n",
    "        ls_preds = []\n",
    "        \n",
    "        logger.info('Making prediction using each %d cv models' % len(self.cb.cv_models))\n",
    "        # get the models used in the cross validations\n",
    "        for cv_model in tqdm(self.cb.get_cv_models()):\n",
    "            # make prediction using combined values with the cb model\n",
    "            pred = cv_model.predict(x_out_cb)\n",
    "            ls_preds.append(pred)\n",
    "        \n",
    "        # compute the mean of the predictions of \n",
    "        #  the cross validation models\n",
    "        return np.mean(ls_preds, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTUbt5C_wY8I"
   },
   "source": [
    "Using this `EnsemblePredictor` and saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rW4gvGmewY8I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlod.file_utils import PredictionStorage\n",
    "\n",
    "# Building the ensemble predictor\n",
    "predictor = EnsemblePredictor(\n",
    "                    lgb_model, \n",
    "                    cb_model, \n",
    "                    (mlod_preprocessor, mlod_pp_opts)\n",
    "                    (airqo_preprocessor, airqo_pp_opts)\n",
    "                )\n",
    "\n",
    "y_test = predictor.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvtJY-PswY8I"
   },
   "outputs": [],
   "source": [
    "# Store the results for submission\n",
    "mean_rmse = np.mean([cb_eval_out['rmse'], lgb_eval_out['rmse']])\n",
    "\n",
    "out_df = pd.DataFrame.from_dict(dict(ID=TEST_IDS.values, target=y_test)).set_index('ID')\n",
    "out_df.to_csv(f'./airqo_sub{mean_rmse}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiolLAXRwY8J"
   },
   "source": [
    "The file to upload should be name `airqo_subXXX.csv`. The values in XXX, is a way for us to keep tabs on training steps with what rmse produces what results.\n",
    "Its also an indicator of ensemble overfitting"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FinalNotebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
